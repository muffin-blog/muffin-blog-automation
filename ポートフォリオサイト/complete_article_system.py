#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å®Œç’§ãªè¨˜äº‹ç®¡ç†ã‚·ã‚¹ãƒ†ãƒ 
- URLã‹ã‚‰æ­£ç¢ºãªãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿å–å¾—
- æŠ€è¡“çš„ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰å®Œå…¨é™¤å»
- Unsplash APIã§ç”»åƒè‡ªå‹•å–å¾—ãƒ»ä¿å­˜
- å…¨è‡ªå‹•åŒ–ãƒ»æ¨æ¸¬ãªã—
"""

import json
import sys
import subprocess
import re
import time
import requests
import os
from datetime import datetime
from urllib.parse import urlparse
from pathlib import Path

# Unsplash APIè¨­å®š
UNSPLASH_ACCESS_KEY = "YOUR_UNSPLASH_ACCESS_KEY"  # å®Ÿéš›ã®ã‚­ãƒ¼ã«ç½®ãæ›ãˆ
UNSPLASH_API_URL = "https://api.unsplash.com/search/photos"

def fetch_url_content(url):
    """URLã‹ã‚‰HTMLã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’å–å¾—"""
    try:
        print(f"ğŸ“¡ URLã‚¢ã‚¯ã‚»ã‚¹ä¸­: {url}")
        result = subprocess.run(['curl', '-s', '-L', '--user-agent', 
                               'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'], 
                               input=url, capture_output=True, text=True)
        if result.returncode == 0 and result.stdout:
            return result.stdout
        else:
            result = subprocess.run(['curl', '-s', '-L', url], capture_output=True, text=True)
            return result.stdout if result.returncode == 0 else None
    except Exception as e:
        print(f"âŒ URLã‚¢ã‚¯ã‚»ã‚¹ã‚¨ãƒ©ãƒ¼: {e}")
        return None

def extract_meta_description(html):
    """HTMLã‹ã‚‰ãƒ¡ã‚¿ãƒ‡ã‚£ã‚¹ã‚¯ãƒªãƒ—ã‚·ãƒ§ãƒ³ã‚’æŠ½å‡º"""
    if not html:
        return ""
    
    patterns = [
        r'<meta\s+name=["\']description["\']\s+content=["\'](.*?)["\']',
        r'<meta\s+content=["\'](.*?)["\']\s+name=["\']description["\']',
        r'<meta\s+property=["\']og:description["\']\s+content=["\'](.*?)["\']',
        r'<meta\s+content=["\'](.*?)["\']\s+property=["\']og:description["\']'
    ]
    
    for pattern in patterns:
        match = re.search(pattern, html, re.IGNORECASE | re.DOTALL)
        if match:
            description = match.group(1).strip()
            description = description.replace('&quot;', '"').replace('&amp;', '&').replace('&lt;', '<').replace('&gt;', '>')
            return description
    
    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: æœ€åˆã®pã‚¿ã‚°
    p_match = re.search(r'<p[^>]*>(.*?)</p>', html, re.IGNORECASE | re.DOTALL)
    if p_match:
        content = re.sub(r'<[^>]+>', '', p_match.group(1))
        content = re.sub(r'\s+', ' ', content).strip()
        if len(content) > 50:
            return content[:200] + '...' if len(content) > 200 else content
    
    return ""

def extract_real_title(html):
    """HTMLã‹ã‚‰å®Ÿéš›ã®ã‚¿ã‚¤ãƒˆãƒ«ã‚’æŠ½å‡º"""
    if not html:
        return ""
    
    pattern = r'<title>(.*?)</title>'
    match = re.search(pattern, html, re.IGNORECASE | re.DOTALL)
    if match:
        title = match.group(1).strip()
        # ã‚µã‚¤ãƒˆåã‚’é™¤å»
        if ' | ' in title:
            title = title.split(' | ')[0]
        if ' - ' in title:
            title = title.split(' - ')[0]
        if ' â€“ ' in title:
            title = title.split(' â€“ ')[0]
        return title
    
    return ""

def extract_clean_keywords(html, title):
    """æŠ€è¡“çš„ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å®Œå…¨é™¤å»ã—ã¦ã‚¯ãƒªãƒ¼ãƒ³ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®ã¿æŠ½å‡º"""
    keywords = set()
    
    if not html or not title:
        return list(keywords)
    
    # script/styleã‚¿ã‚°ã‚’å®Œå…¨é™¤å»
    content = re.sub(r'<script[^>]*>.*?</script>', '', html, flags=re.DOTALL | re.IGNORECASE)
    content = re.sub(r'<style[^>]*>.*?</style>', '', content, flags=re.DOTALL | re.IGNORECASE)
    content = re.sub(r'<[^>]+>', ' ', content)
    content = re.sub(r'\s+', ' ', content)
    
    # æŠ€è¡“çš„ãƒ»ç„¡æ„å‘³ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å¾¹åº•é™¤å»
    exclude_words = {
        # JavaScript/CSSé–¢é€£
        'var', 'function', 'style', 'margin', 'border', 'has', 'preset', 
        'important', 'text', 'ndash', 'document', 'window', 'element', 
        'div', 'span', 'class', 'id', 'background', 'color', 'width', 
        'height', 'padding', 'display', 'position', 'absolute', 'relative', 
        'fixed', 'flex', 'grid', 'const', 'let', 'return', 'false', 'true', 
        'null', 'undefined', 'onclick', 'onload', 'jquery', 'script', 'css', 
        'html', 'src', 'href', 'alt', 'img', 'px', 'rem', 'em',
        
        # ä¸€èˆ¬çš„ãªç„¡æ„å‘³èª
        'ã«ã¤ã„ã¦', 'ã¨ã„ã†', 'ã§ã™ãŒ', 'ã¨ã“ã‚', 'ã“ã¨ãŒ', 'ã•ã‚Œã‚‹', 
        'ã—ã¦ã„ã‚‹', 'ã—ã¾ã™', 'ã¾ã—ãŸ', 'ã§ã™', 'ã§ã‚ã‚‹', 'ã™ã‚‹', 
        'ã—ãŸ', 'ã—ã¦', 'ã•ã›ã‚‹', 'ã‚‰ã‚Œã‚‹', 'ãªã‚‹', 'ã‚ã‚‹', 'ã„ã‚‹',
        'ã“ã®', 'ãã®', 'ã‚ã®', 'ã©ã®', 'ãªã©', 'ã¾ãŸ', 'ã•ã‚‰ã«',
        'ã—ã‹ã—', 'ãŸã ã—', 'ã¤ã¾ã‚Š', 'ãªãŠ', 'ã¡ãªã¿ã«', 'ã§ã¯',
        'ã‹ã‚‰', 'ã¾ã§', 'ã‚ˆã‚Š', 'ã»ã©', 'ãã‚‰ã„', 'ã ã‘', 'ã§ã‚‚',
        'ã‘ã‚Œã©', 'ãã‚Œã§', 'ãã—ã¦', 'ã¾ãŸã¯', 'ã‚‚ã—ãã¯',
        
        # çŸ­ã„ç„¡æ„å‘³ãªæ–‡å­—åˆ—
        'ã§', 'ã«', 'ã‚’', 'ãŒ', 'ã¯', 'ã¨', 'ã‚‚', 'ã®', 'ã‚„', 'ã‹',
        'ã¸', 'ã‚ˆã‚Š', 'ã‹ã‚‰', 'ã¾ã§', 'ã§ã‚‚', 'ãªã‚‰', 'ã‘ã‚Œã©'
    }
    
    # æœ‰åŠ¹ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ‘ã‚¿ãƒ¼ãƒ³
    meaningful_patterns = [
        r'[ã‚¡-ãƒ¶ãƒ¼]{2,}',  # ã‚«ã‚¿ã‚«ãƒŠ2æ–‡å­—ä»¥ä¸Š
        r'[A-Za-z]{4,}',   # ã‚¢ãƒ«ãƒ•ã‚¡ãƒ™ãƒƒãƒˆ4æ–‡å­—ä»¥ä¸Š
        r'[ä¸€-é¾¯]{2,}'     # æ¼¢å­—2æ–‡å­—ä»¥ä¸Š
    ]
    
    # ã‚¿ã‚¤ãƒˆãƒ«ã‹ã‚‰ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡º
    for pattern in meaningful_patterns:
        words = re.findall(pattern, title)
        for word in words:
            if word.lower() not in exclude_words and len(word) >= 2:
                # ç‰¹åˆ¥ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼
                if word not in ['ã¾ã§ãã£ã™ã‚Š', 'ã‘ã‚‹', 'ãˆã‚‹', 'ããªãŒã‚‰', 'ã™ã‚‹ãŸã‚', 'ãŒãŠ', 'ãªã‚‰', 'ã§ã‚‚']:
                    keywords.add(word)
    
    # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‹ã‚‰é‡è¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡ºï¼ˆã‚ˆã‚Šå³ã—ã„æ¡ä»¶ï¼‰
    for pattern in meaningful_patterns:
        words = re.findall(pattern, content)
        word_count = {}
        
        for word in words:
            if (word.lower() not in exclude_words and 
                len(word) >= 2 and 
                word not in ['ã¾ã§ãã£ã™ã‚Š', 'ã‘ã‚‹', 'ãˆã‚‹', 'ããªãŒã‚‰', 'ã™ã‚‹ãŸã‚', 'ãŒãŠ', 'ãªã‚‰', 'ã§ã‚‚']):
                word_count[word] = word_count.get(word, 0) + 1
        
        # 10å›ä»¥ä¸Šå‡ºç¾ã™ã‚‹é‡è¦ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®ã¿
        for word, count in word_count.items():
            if count >= 10:
                keywords.add(word)
    
    # ãƒ‰ãƒ¡ã‚¤ãƒ³ãƒ»URLåŸºæº–ã®ã‚«ãƒ†ã‚´ãƒªã‚¿ã‚°
    if 'audible' in title.lower() or 'ã‚ªãƒ¼ãƒ‡ã‚£ãƒ–ãƒ«' in title:
        keywords.add('ã‚ªãƒ¼ãƒ‡ã‚£ã‚ªãƒ–ãƒƒã‚¯')
        keywords.add('Audible')
    
    if 'sleep' in title.lower() or 'ç¡çœ ' in title:
        keywords.add('ç¡çœ ')
        keywords.add('å¥åº·')
    
    if 'diet' in title.lower() or 'ãƒ€ã‚¤ã‚¨ãƒƒãƒˆ' in title:
        keywords.add('ãƒ€ã‚¤ã‚¨ãƒƒãƒˆ')
        keywords.add('å¥åº·')
    
    return list(keywords)[:6]

def get_unsplash_image(keywords, article_title):
    """Unsplash APIã‹ã‚‰ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹ã§ç”»åƒå–å¾—"""
    try:
        # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’è‹±èªã«å¤‰æ›
        keyword_mapping = {
            'ã‚ªãƒ¼ãƒ‡ã‚£ã‚ªãƒ–ãƒƒã‚¯': 'audiobook headphones',
            'Audible': 'audiobook reading',
            'ç¡çœ ': 'sleep bed',
            'å¥åº·': 'health wellness',
            'ãƒ€ã‚¤ã‚¨ãƒƒãƒˆ': 'diet healthy food',
            'èª­æ›¸': 'reading book',
            'é›†ä¸­åŠ›': 'focus concentration',
            'ã‚¨ã‚¢ã‚³ãƒ³': 'air conditioner cooling',
            'æ•': 'pillow sleep',
            'ãƒãƒƒãƒˆãƒ¬ã‚¹': 'mattress bed'
        }
        
        # æœ€åˆã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’ä½¿ç”¨
        search_query = "book reading"  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
        if keywords:
            for keyword in keywords:
                if keyword in keyword_mapping:
                    search_query = keyword_mapping[keyword]
                    break
        
        print(f"ğŸ–¼ï¸ ç”»åƒæ¤œç´¢: {search_query}")
        
        # å®Ÿéš›ã®Unsplash APIã‚­ãƒ¼ãŒãªã„å ´åˆã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        if UNSPLASH_ACCESS_KEY == "YOUR_UNSPLASH_ACCESS_KEY":
            print("âš ï¸ Unsplash APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆç”»åƒã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
            return "https://images.unsplash.com/photo-1481627834876-b7833e8f5570?w=300&h=200&fit=crop&auto=format"
        
        headers = {"Authorization": f"Client-ID {UNSPLASH_ACCESS_KEY}"}
        params = {
            "query": search_query,
            "per_page": 1,
            "orientation": "landscape"
        }
        
        response = requests.get(UNSPLASH_API_URL, headers=headers, params=params)
        
        if response.status_code == 200:
            data = response.json()
            if data['results']:
                image_url = data['results'][0]['urls']['regular']
                return f"{image_url}?w=300&h=200&fit=crop&auto=format"
        
        return "https://images.unsplash.com/photo-1481627834876-b7833e8f5570?w=300&h=200&fit=crop&auto=format"
        
    except Exception as e:
        print(f"âŒ ç”»åƒå–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
        return "https://images.unsplash.com/photo-1481627834876-b7833e8f5570?w=300&h=200&fit=crop&auto=format"

def get_domain_category(url):
    """URLãƒ‰ãƒ¡ã‚¤ãƒ³ã‹ã‚‰ã‚«ãƒ†ã‚´ãƒªã‚’åˆ¤å®š"""
    domain = urlparse(url).netloc.lower()
    
    if 'muffin-blog.com' in domain:
        return 'ãƒ–ãƒ­ã‚°'
    elif 'minerva-sleep.jp' in domain:
        return 'ç¡çœ ãƒ»å¥åº·'
    elif 'baumclinic.jp' in domain:
        return 'ç¾å®¹ãƒ»ãƒ€ã‚¤ã‚¨ãƒƒãƒˆ'
    elif 'my-best.com' in domain:
        return 'ãƒ¬ãƒ“ãƒ¥ãƒ¼ãƒ»æ¯”è¼ƒ'
    else:
        return 'ãã®ä»–'

def get_complete_article_metadata(url):
    """URLã‹ã‚‰å®Œç’§ãªãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—"""
    try:
        print(f"ğŸ” å®Œå…¨åˆ†æé–‹å§‹: {url}")
        
        html = fetch_url_content(url)
        if not html:
            print(f"âŒ HTMLã®å–å¾—ã«å¤±æ•—: {url}")
            return None
        
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿æŠ½å‡º
        title = extract_real_title(html)
        description = extract_meta_description(html)
        keywords = extract_clean_keywords(html, title)
        category = get_domain_category(url)
        
        # ç”»åƒå–å¾—
        thumbnail = get_unsplash_image(keywords, title)
        
        print(f"âœ… æŠ½å‡ºå®Œäº†:")
        print(f"   ã‚¿ã‚¤ãƒˆãƒ«: {title}")
        print(f"   èª¬æ˜: {description[:50]}..." if description else "   èª¬æ˜: (å–å¾—å¤±æ•—)")
        print(f"   ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {', '.join(keywords)}")
        print(f"   ã‚«ãƒ†ã‚´ãƒª: {category}")
        print(f"   ç”»åƒ: {thumbnail}")
        
        return {
            'title': title,
            'description': description,
            'keywords': keywords,
            'category': category,
            'thumbnail': thumbnail
        }
        
    except Exception as e:
        print(f"âŒ ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
        return None

def clean_existing_tags():
    """æ—¢å­˜è¨˜äº‹ã®ã‚¿ã‚°ã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
    try:
        print("ğŸ§¹ æ—¢å­˜ã‚¿ã‚°ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—é–‹å§‹")
        
        json_path = "public/content/articles/articles.json"
        with open(json_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        # æŠ€è¡“çš„ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’é™¤å»
        tech_keywords = {
            'var', 'function', 'style', 'margin', 'border', 'has', 'preset',
            'important', 'text', 'ndash', 'ã¾ã§ãã£ã™ã‚Š', 'ã‘ã‚‹', 'ãˆã‚‹',
            'ããªãŒã‚‰', 'ã™ã‚‹ãŸã‚', 'ãŒãŠ', 'ãªã‚‰', 'ã§ã‚‚', 'ã¸ã®', 'ã™ã‚‹',
            'ã—ãŸ', 'ã—ã¦', 'ã•ã‚Œã‚‹', 'ã—ã¦ã„ã‚‹', 'ã—ã¾ã™', 'ã¾ã—ãŸ'
        }
        
        # SEOè¨˜äº‹ã®ã‚¿ã‚°ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        for article in data.get('seoArticles', []):
            if 'tags' in article:
                cleaned_tags = [tag for tag in article['tags'] if tag not in tech_keywords]
                article['tags'] = cleaned_tags[:6]
        
        # ãƒ–ãƒ­ã‚°è¨˜äº‹ã®ã‚¿ã‚°ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        for article in data.get('blogArticles', []):
            if 'tags' in article:
                cleaned_tags = [tag for tag in article['tags'] if tag not in tech_keywords]
                article['tags'] = cleaned_tags[:6]
        
        # æ›´æ–°æ—¥æ™‚è¨­å®š
        data['_lastUpdate'] = datetime.now().strftime('%Y-%m-%dT%H:%M:%S+09:00')
        data['_cacheBreaker'] = int(time.time())
        
        # ä¿å­˜
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        
        print("âœ… ã‚¿ã‚°ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å®Œäº†")
        return True
        
    except Exception as e:
        print(f"âŒ ã‚¿ã‚°ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã‚¨ãƒ©ãƒ¼: {e}")
        return False

def add_new_article(url):
    """æ–°è¨˜äº‹ã‚’è¿½åŠ """
    try:
        print(f"ğŸ¯ æ–°è¨˜äº‹è¿½åŠ é–‹å§‹: {url}")
        
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿å–å¾—
        metadata = get_complete_article_metadata(url)
        if not metadata:
            return False
        
        # è¨˜äº‹ãƒ‡ãƒ¼ã‚¿æ§‹ç¯‰
        article_data = {
            "title": metadata['title'],
            "url": url,
            "description": metadata['description'],
            "date": datetime.now().strftime('%Y-%m-%d'),
            "tags": [metadata['category']] + metadata['keywords'],
            "client": "Muffin Blog" if 'muffin-blog.com' in url else metadata['category'],
            "thumbnail": metadata['thumbnail']
        }
        
        # articles.jsonæ›´æ–°
        json_path = "public/content/articles/articles.json"
        with open(json_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        # é‡è¤‡ãƒã‚§ãƒƒã‚¯
        target_section = 'blogArticles' if 'muffin-blog.com' in url else 'seoArticles'
        for existing in data[target_section]:
            if existing['url'] == url:
                print("âš ï¸ åŒã˜URLã®è¨˜äº‹ãŒæ—¢ã«å­˜åœ¨ã—ã¾ã™")
                return False
        
        # å…ˆé ­ã«è¿½åŠ 
        data[target_section].insert(0, article_data)
        
        # æ›´æ–°æ—¥æ™‚è¨­å®š
        data['_lastUpdate'] = datetime.now().strftime('%Y-%m-%dT%H:%M:%S+09:00')
        data['_cacheBreaker'] = int(time.time())
        
        # ä¿å­˜
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        
        print("âœ… è¨˜äº‹è¿½åŠ å®Œäº†")
        return True
        
    except Exception as e:
        print(f"âŒ è¨˜äº‹è¿½åŠ ã‚¨ãƒ©ãƒ¼: {e}")
        return False

def deploy_to_vercel():
    """Vercelã«ãƒ‡ãƒ—ãƒ­ã‚¤"""
    try:
        print("ğŸš€ Gitã‚³ãƒŸãƒƒãƒˆä¸­...")
        subprocess.run(['git', 'add', '.'], check=True)
        
        commit_msg = f"å®Œç’§ãªè¨˜äº‹ã‚·ã‚¹ãƒ†ãƒ æ›´æ–°: {datetime.now().strftime('%Y-%m-%d %H:%M')}"
        subprocess.run(['git', 'commit', '-m', commit_msg], check=True)
        
        subprocess.run(['git', 'push', 'origin', 'master'], check=True)
        print("âœ… Gitãƒ—ãƒƒã‚·ãƒ¥å®Œäº†")
        
        print("ğŸš€ Vercelãƒ‡ãƒ—ãƒ­ã‚¤ä¸­...")
        result = subprocess.run(['npx', 'vercel', '--prod'], capture_output=True, text=True)
        print("âœ… Vercelãƒ‡ãƒ—ãƒ­ã‚¤å®Œäº†")
        
        return True
        
    except subprocess.CalledProcessError as e:
        print(f"âŒ ãƒ‡ãƒ—ãƒ­ã‚¤ã‚¨ãƒ©ãƒ¼: {e}")
        return False

def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    print("ğŸ¯ å®Œç’§ãªè¨˜äº‹ç®¡ç†ã‚·ã‚¹ãƒ†ãƒ ")
    print("=" * 60)
    
    if len(sys.argv) == 1:
        # æ—¢å­˜è¨˜äº‹ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã®ã¿
        print("ğŸ“ æ—¢å­˜è¨˜äº‹ã®ã‚¿ã‚°ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã‚’å®Ÿè¡Œ")
        if clean_existing_tags():
            if deploy_to_vercel():
                print("ğŸ‰ ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å®Œäº†ï¼")
            else:
                print("âŒ ãƒ‡ãƒ—ãƒ­ã‚¤ã«å¤±æ•—")
        else:
            print("âŒ ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã«å¤±æ•—")
    
    elif len(sys.argv) == 2:
        # æ–°è¨˜äº‹è¿½åŠ 
        url = sys.argv[1]
        print(f"ğŸ“ æ–°è¨˜äº‹è¿½åŠ : {url}")
        
        # ã¾ãšã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        clean_existing_tags()
        
        # æ–°è¨˜äº‹è¿½åŠ 
        if add_new_article(url):
            if deploy_to_vercel():
                print("ğŸ‰ è¨˜äº‹è¿½åŠ ã¨ãƒ‡ãƒ—ãƒ­ã‚¤å®Œäº†ï¼")
                print(f"ğŸ“± ã‚µã‚¤ãƒˆç¢ºèª: https://muffin-portfolio-public.vercel.app")
            else:
                print("âŒ ãƒ‡ãƒ—ãƒ­ã‚¤ã«å¤±æ•—")
        else:
            print("âŒ è¨˜äº‹è¿½åŠ ã«å¤±æ•—")
    
    else:
        print("âŒ ä½¿ç”¨æ–¹æ³•:")
        print("  æ—¢å­˜ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—: python3 complete_article_system.py")
        print("  æ–°è¨˜äº‹è¿½åŠ : python3 complete_article_system.py [è¨˜äº‹URL]")

if __name__ == "__main__":
    main()