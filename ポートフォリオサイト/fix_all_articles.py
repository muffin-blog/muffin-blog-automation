#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å…¨è¨˜äº‹ã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿è‡ªå‹•ä¿®æ­£ã‚·ã‚¹ãƒ†ãƒ 
- URLã‹ã‚‰å®Ÿéš›ã®ãƒ¡ã‚¿ãƒ‡ã‚£ã‚¹ã‚¯ãƒªãƒ—ã‚·ãƒ§ãƒ³ã€ã‚¿ã‚°ã€ã‚«ãƒ†ã‚´ãƒªã‚’è‡ªå‹•å–å¾—
- ã™ã¹ã¦ã®è¨˜äº‹ã‚’æ­£ã—ã„ãƒ‡ãƒ¼ã‚¿ã§æ›´æ–°
- æ¨æ¸¬ãªã—ã€å®Œå…¨è‡ªå‹•åŒ–
"""

import json
import sys
import subprocess
import re
import time
from datetime import datetime
from urllib.parse import urlparse

def fetch_url_content(url):
    """URLã‹ã‚‰HTMLã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’å–å¾—"""
    try:
        print(f"ğŸ“¡ URLã‚¢ã‚¯ã‚»ã‚¹ä¸­: {url}")
        result = subprocess.run(['curl', '-s', '-L', '--user-agent', 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'], 
                               input=url, capture_output=True, text=True)
        if result.returncode == 0 and result.stdout:
            return result.stdout
        else:
            # fallback: curlã‚³ãƒãƒ³ãƒ‰ã‚’ç›´æ¥å®Ÿè¡Œ
            result = subprocess.run(['curl', '-s', '-L', url], capture_output=True, text=True)
            return result.stdout if result.returncode == 0 else None
    except Exception as e:
        print(f"âŒ URLã‚¢ã‚¯ã‚»ã‚¹ã‚¨ãƒ©ãƒ¼: {e}")
        return None

def extract_meta_description(html):
    """HTMLã‹ã‚‰ãƒ¡ã‚¿ãƒ‡ã‚£ã‚¹ã‚¯ãƒªãƒ—ã‚·ãƒ§ãƒ³ã‚’æŠ½å‡º"""
    if not html:
        return ""
    
    # meta name="description" ã‚’æ¤œç´¢
    patterns = [
        r'<meta\s+name=["\']description["\']\s+content=["\'](.*?)["\']',
        r'<meta\s+content=["\'](.*?)["\']\s+name=["\']description["\']',
        r'<meta\s+property=["\']og:description["\']\s+content=["\'](.*?)["\']',
        r'<meta\s+content=["\'](.*?)["\']\s+property=["\']og:description["\']'
    ]
    
    for pattern in patterns:
        match = re.search(pattern, html, re.IGNORECASE | re.DOTALL)
        if match:
            description = match.group(1).strip()
            # HTMLã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã‚’ãƒ‡ã‚³ãƒ¼ãƒ‰
            description = description.replace('&quot;', '"').replace('&amp;', '&').replace('&lt;', '<').replace('&gt;', '>')
            return description
    
    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: æœ€åˆã®pã‚¿ã‚°ã®å†…å®¹ã‚’å–å¾—
    p_match = re.search(r'<p[^>]*>(.*?)</p>', html, re.IGNORECASE | re.DOTALL)
    if p_match:
        content = re.sub(r'<[^>]+>', '', p_match.group(1))
        content = re.sub(r'\s+', ' ', content).strip()
        if len(content) > 50:  # ååˆ†ãªé•·ã•ãŒã‚ã‚‹å ´åˆã®ã¿
            return content[:200] + '...' if len(content) > 200 else content
    
    return ""

def extract_real_title(html):
    """HTMLã‹ã‚‰å®Ÿéš›ã®ã‚¿ã‚¤ãƒˆãƒ«ã‚’æŠ½å‡º"""
    if not html:
        return ""
    
    # <title>ã‚¿ã‚°ã‹ã‚‰æŠ½å‡º
    pattern = r'<title>(.*?)</title>'
    match = re.search(pattern, html, re.IGNORECASE | re.DOTALL)
    if match:
        title = match.group(1).strip()
        # ã‚µã‚¤ãƒˆåã‚’é™¤å»
        if ' | ' in title:
            title = title.split(' | ')[0]
        if ' - ' in title:
            title = title.split(' - ')[0]
        return title
    
    return ""

def extract_keywords_from_content(html, title):
    """ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‹ã‚‰å®Ÿéš›ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡º"""
    keywords = set()
    
    if not html or not title:
        return list(keywords)
    
    # HTMLã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’æŠ½å‡ºï¼ˆscript/styleã‚¿ã‚°ã‚’é™¤å»ï¼‰
    content = re.sub(r'<script[^>]*>.*?</script>', '', html, flags=re.DOTALL | re.IGNORECASE)
    content = re.sub(r'<style[^>]*>.*?</style>', '', content, flags=re.DOTALL | re.IGNORECASE)
    content = re.sub(r'<[^>]+>', ' ', content)
    content = re.sub(r'\s+', ' ', content)
    
    # é™¤å¤–ã™ã‚‹æŠ€è¡“çš„ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
    exclude_words = {
        'var', 'function', 'style', 'margin', 'border', 'has', 'preset', 
        'important', 'text', 'ndash', 'ã«ã¤ã„ã¦', 'ã¨ã„ã†', 'ã§ã™ãŒ', 
        'ã¨ã“ã‚', 'ã“ã¨ãŒ', 'ã•ã‚Œã‚‹', 'ã—ã¦ã„ã‚‹', 'ã—ã¾ã™', 'ã¾ã—ãŸ',
        'document', 'window', 'element', 'div', 'span', 'class', 'id',
        'background', 'color', 'width', 'height', 'padding', 'display',
        'position', 'absolute', 'relative', 'fixed', 'flex', 'grid'
    }
    
    # ã‚¿ã‚¤ãƒˆãƒ«ã‹ã‚‰ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡º
    title_words = re.findall(r'[ã‚¡-ãƒ¶ãƒ¼]{2,}|[ã‚-ã‚“]{2,}|[ä¸€-é¾¯]{2,}|[A-Za-z]{3,}', title)
    for word in title_words:
        if word.lower() not in exclude_words and len(word) >= 2:
            keywords.add(word)
    
    # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‹ã‚‰æ„å‘³ã®ã‚ã‚‹ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡º
    words = re.findall(r'[ã‚¡-ãƒ¶ãƒ¼]{2,}|[ã‚-ã‚“]{2,}|[ä¸€-é¾¯]{2,}|[A-Za-z]{3,}', content)
    word_count = {}
    for word in words:
        if word.lower() not in exclude_words and len(word) >= 2:
            word_count[word] = word_count.get(word, 0) + 1
    
    # é »å‡ºä¸Šä½ã‚’è¿½åŠ ï¼ˆã‚ˆã‚Šé«˜ã„é–¾å€¤ã‚’è¨­å®šï¼‰
    sorted_words = sorted(word_count.items(), key=lambda x: x[1], reverse=True)
    for word, count in sorted_words[:6]:  # ä¸Šä½6å€‹
        if count >= 3 and len(word) >= 2:  # 3å›ä»¥ä¸Šå‡ºç¾
            keywords.add(word)
    
    return list(keywords)[:6]  # æœ€å¤§6å€‹

def extract_category_from_url(url):
    """URLã‹ã‚‰ã‚«ãƒ†ã‚´ãƒªã‚’æ¨å®š"""
    parsed = urlparse(url)
    path_parts = parsed.path.strip('/').split('/')
    
    # URLãƒ‘ã‚¹ã‹ã‚‰ã‚«ãƒ†ã‚´ãƒªã‚’æ¨å®š
    for part in path_parts:
        if part in ['audible', 'audiobook', 'ã‚ªãƒ¼ãƒ‡ã‚£ãƒ–ãƒ«']:
            return 'ã‚ªãƒ¼ãƒ‡ã‚£ã‚ªãƒ–ãƒƒã‚¯'
        elif part in ['seo', 'marketing']:
            return 'SEOãƒ»ãƒãƒ¼ã‚±ãƒ†ã‚£ãƒ³ã‚°'
        elif part in ['health', 'beauty', 'å¥åº·', 'ç¾å®¹']:
            return 'å¥åº·ãƒ»ç¾å®¹'
        elif part in ['tech', 'technology', 'æŠ€è¡“']:
            return 'ãƒ†ã‚¯ãƒãƒ­ã‚¸ãƒ¼'
        elif part in ['finance', 'é‡‘è', 'æŠ•è³‡']:
            return 'é‡‘èãƒ»æŠ•è³‡'
        elif part in ['lifestyle', 'life', 'ãƒ©ã‚¤ãƒ•ã‚¹ã‚¿ã‚¤ãƒ«']:
            return 'ãƒ©ã‚¤ãƒ•ã‚¹ã‚¿ã‚¤ãƒ«'
    
    # ãƒ‰ãƒ¡ã‚¤ãƒ³ã‹ã‚‰ã‚«ãƒ†ã‚´ãƒªã‚’æ¨å®š
    domain = parsed.netloc
    if 'muffin-blog.com' in domain:
        return 'ãƒ–ãƒ­ã‚°'
    elif 'minerva-sleep.jp' in domain:
        return 'ç¡çœ ãƒ»å¥åº·'
    elif 'baumclinic.jp' in domain:
        return 'ç¾å®¹ãƒ»ãƒ€ã‚¤ã‚¨ãƒƒãƒˆ'
    elif 'my-best.com' in domain:
        return 'ãƒ¬ãƒ“ãƒ¥ãƒ¼ãƒ»æ¯”è¼ƒ'
    
    return 'ãã®ä»–'

def get_article_metadata(url):
    """URLã‹ã‚‰å®Œå…¨ãªãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—"""
    try:
        print(f"ğŸ” è¨˜äº‹åˆ†æé–‹å§‹: {url}")
        
        html = fetch_url_content(url)
        if not html:
            print(f"âŒ HTMLã®å–å¾—ã«å¤±æ•—: {url}")
            return None
        
        # å„ç¨®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º
        title = extract_real_title(html)
        description = extract_meta_description(html)
        keywords = extract_keywords_from_content(html, title)
        category = extract_category_from_url(url)
        
        print(f"âœ… æŠ½å‡ºå®Œäº†:")
        print(f"   ã‚¿ã‚¤ãƒˆãƒ«: {title}")
        print(f"   èª¬æ˜: {description[:50]}..." if description else "   èª¬æ˜: (å–å¾—å¤±æ•—)")
        print(f"   ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {', '.join(keywords)}")
        print(f"   ã‚«ãƒ†ã‚´ãƒª: {category}")
        
        return {
            'title': title,
            'description': description,
            'keywords': keywords,
            'category': category
        }
        
    except Exception as e:
        print(f"âŒ ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
        return None

def update_article_data(article, metadata):
    """è¨˜äº‹ãƒ‡ãƒ¼ã‚¿ã‚’æ–°ã—ã„ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã§æ›´æ–°"""
    if not metadata:
        return article
    
    # ã‚¿ã‚¤ãƒˆãƒ«ã‚’æ›´æ–°ï¼ˆå…ƒã®ã‚¿ã‚¤ãƒˆãƒ«ãŒç©ºã‚„ä¸æ­£ãªå ´åˆã®ã¿ï¼‰
    if metadata['title'] and (not article.get('title') or len(article['title']) < 10):
        article['title'] = metadata['title']
    
    # èª¬æ˜ã‚’æ›´æ–°ï¼ˆå¸¸ã«å®Ÿéš›ã®èª¬æ˜ã§ä¸Šæ›¸ãï¼‰
    if metadata['description']:
        article['description'] = metadata['description']
    
    # ã‚¿ã‚°ã‚’æ›´æ–°ï¼ˆå®Ÿéš›ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã§ç½®ãæ›ãˆï¼‰
    if metadata['keywords']:
        # æ—¢å­˜ã®ã‚¿ã‚°ã¨æ–°ã—ã„ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’ãƒãƒ¼ã‚¸
        existing_tags = set(article.get('tags', []))
        new_tags = set(metadata['keywords'])
        combined_tags = list(existing_tags.union(new_tags))[:8]  # æœ€å¤§8å€‹
        article['tags'] = combined_tags
    
    # ã‚«ãƒ†ã‚´ãƒªã‚’è¿½åŠ 
    if metadata['category'] and metadata['category'] not in article.get('tags', []):
        if 'tags' not in article:
            article['tags'] = []
        article['tags'].insert(0, metadata['category'])
        article['tags'] = article['tags'][:8]  # æœ€å¤§8å€‹ã«åˆ¶é™
    
    return article

def fix_all_articles():
    """ã™ã¹ã¦ã®è¨˜äº‹ã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’ä¿®æ­£"""
    try:
        print("ğŸš€ å…¨è¨˜äº‹ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä¿®æ­£é–‹å§‹")
        print("-" * 60)
        
        # articles.jsonã‚’èª­ã¿è¾¼ã¿
        json_path = "public/content/articles/articles.json"
        with open(json_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        print(f"ğŸ“Š è¨˜äº‹çµ±è¨ˆ:")
        print(f"   SEOè¨˜äº‹: {len(data.get('seoArticles', []))}ä»¶")
        print(f"   ãƒ–ãƒ­ã‚°è¨˜äº‹: {len(data.get('blogArticles', []))}ä»¶")
        print("-" * 60)
        
        # SEOè¨˜äº‹ã‚’ä¿®æ­£
        if 'seoArticles' in data:
            print("ğŸ”§ SEOè¨˜äº‹ã®ä¿®æ­£ä¸­...")
            for i, article in enumerate(data['seoArticles']):
                print(f"\n[{i+1}/{len(data['seoArticles'])}] {article.get('title', 'ã‚¿ã‚¤ãƒˆãƒ«ä¸æ˜')}")
                metadata = get_article_metadata(article['url'])
                data['seoArticles'][i] = update_article_data(article, metadata)
                time.sleep(1)  # ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾ç­–
        
        # ãƒ–ãƒ­ã‚°è¨˜äº‹ã‚’ä¿®æ­£
        if 'blogArticles' in data:
            print("\nğŸ”§ ãƒ–ãƒ­ã‚°è¨˜äº‹ã®ä¿®æ­£ä¸­...")
            for i, article in enumerate(data['blogArticles']):
                print(f"\n[{i+1}/{len(data['blogArticles'])}] {article.get('title', 'ã‚¿ã‚¤ãƒˆãƒ«ä¸æ˜')}")
                metadata = get_article_metadata(article['url'])
                data['blogArticles'][i] = update_article_data(article, metadata)
                time.sleep(1)  # ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾ç­–
        
        # æ›´æ–°æ—¥æ™‚ã‚’è¨­å®š
        data['_lastUpdate'] = datetime.now().strftime('%Y-%m-%dT%H:%M:%S+09:00')
        data['_cacheBreaker'] = int(time.time())
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        
        print("\n" + "=" * 60)
        print("ğŸ‰ å…¨è¨˜äº‹ä¿®æ­£å®Œäº†ï¼")
        print(f"ğŸ“ æ›´æ–°æ—¥æ™‚: {data['_lastUpdate']}")
        print(f"ğŸ”„ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ–ãƒ¬ãƒ¼ã‚«ãƒ¼: {data['_cacheBreaker']}")
        print("=" * 60)
        
        return True
        
    except Exception as e:
        print(f"âŒ å…¨è¨˜äº‹ä¿®æ­£ã‚¨ãƒ©ãƒ¼: {e}")
        return False

def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    print("ğŸ¯ å…¨è¨˜äº‹ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿è‡ªå‹•ä¿®æ­£ã‚·ã‚¹ãƒ†ãƒ ")
    print("=" * 60)
    
    if not fix_all_articles():
        print("âŒ å‡¦ç†ã«å¤±æ•—ã—ã¾ã—ãŸ")
        sys.exit(1)
    
    print("\nğŸš€ Gitã‚³ãƒŸãƒƒãƒˆä¸­...")
    try:
        subprocess.run(['git', 'add', 'public/content/articles/articles.json'], check=True)
        commit_msg = f"å…¨è¨˜äº‹ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿è‡ªå‹•ä¿®æ­£: {datetime.now().strftime('%Y-%m-%d %H:%M')}"
        subprocess.run(['git', 'commit', '-m', commit_msg], check=True)
        print("âœ… Gitã‚³ãƒŸãƒƒãƒˆå®Œäº†")
    except subprocess.CalledProcessError as e:
        print(f"âš ï¸ Gitã‚³ãƒŸãƒƒãƒˆå¤±æ•—: {e}")
    
    print("\nğŸ¯ å‡¦ç†å®Œäº†ï¼ã™ã¹ã¦ã®è¨˜äº‹ãŒæ­£ã—ã„ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã§æ›´æ–°ã•ã‚Œã¾ã—ãŸã€‚")

if __name__ == "__main__":
    main()