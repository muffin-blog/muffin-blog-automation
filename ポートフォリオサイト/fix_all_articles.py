#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å…¨è¨˜äº‹ãƒ‡ãƒ¼ã‚¿ã‚’å®Œå…¨ä¿®æ­£
- 404ã‚¨ãƒ©ãƒ¼è¨˜äº‹ã‚’å‰Šé™¤
- SEOè¨˜äº‹ã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’è‡ªå‹•å–å¾—
- å…¨è¨˜äº‹ã®ç”»åƒURLã‚’é©åˆ‡ã«å–å¾—
"""

import json
import requests
from urllib.parse import urlparse
import re

class ArticlesFixer:
    def __init__(self):
        self.articles_path = 'public/content/articles/articles.json'
        
    def check_url_status(self, url):
        """URLã®æœ‰åŠ¹æ€§ã‚’ãƒã‚§ãƒƒã‚¯"""
        try:
            response = requests.head(url, allow_redirects=True, timeout=5)
            return response.status_code != 404
        except:
            return False
    
    def get_meta_from_html(self, url):
        """HTMLã‹ã‚‰ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—"""
        try:
            response = requests.get(url, timeout=10)
            if response.status_code != 200:
                return None
                
            html = response.text
            
            # ã‚¿ã‚¤ãƒˆãƒ«å–å¾—
            title_match = re.search(r'<title[^>]*>([^<]+)</title>', html, re.IGNORECASE)
            title = title_match.group(1) if title_match else ""
            # ã‚µã‚¤ãƒˆåã‚’é™¤å»
            if ' | ' in title:
                title = title.split(' | ')[0]
            elif ' - ' in title:
                title = title.split(' - ')[0]
            
            # ãƒ¡ã‚¿ãƒ‡ã‚£ã‚¹ã‚¯ãƒªãƒ—ã‚·ãƒ§ãƒ³å–å¾—ï¼ˆçŸ­ã„å½¢å¼ï¼‰
            desc_match = re.search(r'<meta\s+(?:name="description"\s+content="([^"]+)"|content="([^"]+)"\s+name="description")', html, re.IGNORECASE)
            description = desc_match.group(1) or desc_match.group(2) if desc_match else ""
            
            # 150æ–‡å­—ã«åˆ¶é™
            if len(description) > 150:
                description = description[:147] + "..."
            
            # OGPç”»åƒå–å¾—
            og_image_match = re.search(r'<meta\s+(?:property="og:image"\s+content="([^"]+)"|content="([^"]+)"\s+property="og:image")', html, re.IGNORECASE)
            thumbnail = og_image_match.group(1) or og_image_match.group(2) if og_image_match else None
            
            # æ—¥ä»˜å–å¾—
            date_match = re.search(r'"datePublished"\s*:\s*"(\d{4}-\d{2}-\d{2})', html)
            if not date_match:
                date_match = re.search(r'<time[^>]*datetime="(\d{4}-\d{2}-\d{2})', html)
            date = date_match.group(1) if date_match else "2024-01-01"
            
            return {
                'title': title,
                'description': description,
                'thumbnail': thumbnail,
                'date': date
            }
        except Exception as e:
            print(f"âŒ ã‚¨ãƒ©ãƒ¼: {url} - {e}")
            return None
    
    def extract_domain_name(self, url):
        """URLã‹ã‚‰ãƒ‰ãƒ¡ã‚¤ãƒ³åã‚’æŠ½å‡ºã—ã¦ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåã‚’æ¨æ¸¬"""
        domain = urlparse(url).netloc
        
        if 'minerva-sleep' in domain:
            return 'ãƒŸãƒãƒ«ãƒ´ã‚¡ã‚¹ãƒªãƒ¼ãƒ—'
        elif 'my-best.com' in domain:
            return 'ãƒã‚¤ãƒ™ã‚¹ãƒˆ'
        elif 'baumclinic' in domain:
            return 'ãƒã‚¦ãƒ ã‚¯ãƒªãƒ‹ãƒƒã‚¯'
        elif 'note.com' in domain:
            return 'note'
        else:
            # ãƒ‰ãƒ¡ã‚¤ãƒ³åã‹ã‚‰æ¨æ¸¬
            name = domain.replace('www.', '').split('.')[0]
            return name.title()
    
    def generate_tags_from_content(self, title, description, url):
        """ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‹ã‚‰ã‚¿ã‚°ã‚’ç”Ÿæˆ"""
        tags = []
        content = (title + " " + description).lower()
        
        # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒãƒƒãƒ”ãƒ³ã‚°
        tag_map = {
            'ç¡çœ ': 'ç¡çœ ',
            'ãƒãƒƒãƒˆãƒ¬ã‚¹': 'ãƒãƒƒãƒˆãƒ¬ã‚¹',
            'æ•': 'æ•',
            'ãƒ€ã‚¤ã‚¨ãƒƒãƒˆ': 'ãƒ€ã‚¤ã‚¨ãƒƒãƒˆ',
            'å¥åº·': 'å¥åº·',
            'ç¾å®¹': 'ç¾å®¹',
            'ãƒ¢ãƒã‚¤ãƒ«': 'ãƒ¢ãƒã‚¤ãƒ«',
            'uq': 'UQãƒ¢ãƒã‚¤ãƒ«',
            'æŠ•è³‡': 'æŠ•è³‡',
            'ç¯€ç´„': 'ç¯€ç´„',
            'seo': 'SEO'
        }
        
        for keyword, tag in tag_map.items():
            if keyword in content and tag not in tags:
                tags.append(tag)
        
        # URLã‹ã‚‰ã‚‚ã‚¿ã‚°ç”Ÿæˆ
        if 'sleep' in url or 'minerva' in url:
            if 'ç¡çœ ' not in tags:
                tags.append('ç¡çœ ')
        
        return tags[:5]  # æœ€å¤§5å€‹
    
    def fix_all_articles(self):
        """å…¨è¨˜äº‹ã‚’ä¿®æ­£"""
        print("ğŸ”§ å…¨è¨˜äº‹ãƒ‡ãƒ¼ã‚¿ä¿®æ­£é–‹å§‹...")
        
        # æ—¢å­˜ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
        with open(self.articles_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        # SEOè¨˜äº‹ã‚’ä¿®æ­£
        print("\nğŸ“Š SEOè¨˜äº‹ã‚’ä¿®æ­£ä¸­...")
        fixed_seo = []
        for article in data['seoArticles']:
            url = article['url']
            print(f"ğŸ” ç¢ºèªä¸­: {url}")
            
            # URLæœ‰åŠ¹æ€§ãƒã‚§ãƒƒã‚¯
            if not self.check_url_status(url):
                print(f"âŒ 404ã‚¨ãƒ©ãƒ¼: ã‚¹ã‚­ãƒƒãƒ—")
                continue
            
            # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿å–å¾—
            meta = self.get_meta_from_html(url)
            if meta:
                # æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã‚’æ›´æ–°
                article['title'] = meta['title'] or article.get('title', '')
                article['description'] = meta['description'] or article.get('description', '')[:150]
                article['thumbnail'] = meta['thumbnail'] or article.get('thumbnail')
                article['date'] = meta['date'] or article.get('date', '2024-01-01')
                
                # ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåè¨­å®š
                if 'client' not in article or not article['client']:
                    article['client'] = self.extract_domain_name(url)
                
                # ã‚¿ã‚°ç”Ÿæˆ
                article['tags'] = self.generate_tags_from_content(
                    article['title'], 
                    article['description'],
                    url
                )
                
                print(f"âœ… ä¿®æ­£å®Œäº†: {article['title'][:30]}...")
            else:
                print(f"âš ï¸ ãƒ¡ã‚¿å–å¾—å¤±æ•—ã€æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ä¿æŒ")
            
            fixed_seo.append(article)
        
        # ãƒ–ãƒ­ã‚°è¨˜äº‹ã‚’ä¿®æ­£ï¼ˆ404ãƒã‚§ãƒƒã‚¯ï¼‰
        print("\nğŸ“ ãƒ–ãƒ­ã‚°è¨˜äº‹ã‚’ç¢ºèªä¸­...")
        fixed_blog = []
        for article in data['blogArticles']:
            url = article['url']
            
            # 404ã‚¨ãƒ©ãƒ¼è¨˜äº‹ã‚’é™¤å¤–
            if '404' in article.get('title', '') or 'ãƒšãƒ¼ã‚¸ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“' in article.get('title', ''):
                print(f"ğŸ—‘ï¸ 404è¨˜äº‹ã‚’å‰Šé™¤: {url}")
                continue
            
            # URLæœ‰åŠ¹æ€§ãƒã‚§ãƒƒã‚¯
            if url != 'https://muffin-blog.com/' and not self.check_url_status(url):
                print(f"âŒ 404ã‚¨ãƒ©ãƒ¼: {url} - ã‚¹ã‚­ãƒƒãƒ—")
                continue
            
            fixed_blog.append(article)
            print(f"âœ… ä¿æŒ: {article['title'][:30]}...")
        
        # ãƒ‡ãƒ¼ã‚¿æ›´æ–°
        data['seoArticles'] = fixed_seo
        data['blogArticles'] = fixed_blog
        
        # ä¿å­˜
        with open(self.articles_path, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ‰ ä¿®æ­£å®Œäº†ï¼")
        print(f"   SEOè¨˜äº‹: {len(fixed_seo)}ä»¶")
        print(f"   ãƒ–ãƒ­ã‚°è¨˜äº‹: {len(fixed_blog)}ä»¶")
        print(f"   å‰Šé™¤: {len(data.get('seoArticles', [])) + len(data.get('blogArticles', [])) - len(fixed_seo) - len(fixed_blog)}ä»¶")

if __name__ == "__main__":
    fixer = ArticlesFixer()
    fixer.fix_all_articles()