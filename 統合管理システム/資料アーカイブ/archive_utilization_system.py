#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è‡ªå‹•æ¤œç´¢ãƒ»æ´»ç”¨ã‚·ã‚¹ãƒ†ãƒ  - çµ±åˆè³‡æ–™ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–é€£æº
ä¿¡é ¼ã§ãã‚‹ã‚½ãƒ¼ã‚¹æƒ…å ±ã®è‡ªå‹•ç™ºè¦‹ãƒ»æ´»ç”¨ãƒ»å¾ªç’°ã‚·ã‚¹ãƒ†ãƒ 

çµ±åˆæ©Ÿèƒ½:
- ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–è‡ªå‹•æ¤œç´¢
- é‡è¤‡ãƒã‚§ãƒƒã‚¯ãƒ»é˜²æ­¢
- å·®åˆ¥åŒ–ææ¡ˆè‡ªå‹•ç”Ÿæˆ
- å“è³ªå‘ä¸Šè¦ç´ è‡ªå‹•æŠ½å‡º
- æ–°çŸ¥è­˜è‡ªå‹•è“„ç©
"""

import os
import re
import json
from datetime import datetime
from typing import Dict, List, Optional, Tuple, Any
from pathlib import Path
import difflib

class ArchiveUtilizationSystem:
    """çµ±åˆè³‡æ–™ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–è‡ªå‹•æ¤œç´¢ãƒ»æ´»ç”¨ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self):
        """åˆæœŸåŒ–"""
        self.base_path = "/Users/satoumasamitsu/Desktop/osigoto/"
        self.archive_path = os.path.join(self.base_path, "çµ±åˆç®¡ç†ã‚·ã‚¹ãƒ†ãƒ /è³‡æ–™ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–/")
        self.notebook_path = os.path.join(self.base_path, "ãƒ–ãƒ­ã‚°è‡ªå‹•åŒ–/NotebookLMè³‡æ–™/")
        self.duplication_db_path = os.path.join(self.archive_path, "ä½œæˆæ¸ˆã¿è¨˜äº‹ãƒ»ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰é‡è¤‡ç®¡ç†.md")
        
        # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒãƒƒãƒ”ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹
        self.keyword_categories = {
            "audible": {
                "primary": ["Audible", "ã‚ªãƒ¼ãƒ‡ã‚£ãƒ–ãƒ«", "è´ãèª­æ›¸"],
                "secondary": ["æ–™é‡‘", "ãƒ—ãƒ©ãƒ³", "é›†ä¸­åŠ›", "èª­è§£åŠ›", "åŠ¹æœ"],
                "tertiary": ["ä½¿ã„æ–¹", "æ´»ç”¨æ³•", "ãƒ¡ãƒªãƒƒãƒˆ", "ãƒ‡ãƒ¡ãƒªãƒƒãƒˆ", "æ¯”è¼ƒ"]
            },
            "audiobook": {
                "primary": ["ã‚ªãƒ¼ãƒ‡ã‚£ã‚ªãƒ–ãƒƒã‚¯", "audiobook.jp", "è´ãæ”¾é¡Œ"],
                "secondary": ["èª­æ›¸è¡“", "åŠ¹æœ", "æ–¹æ³•", "æ¯”è¼ƒ", "æ–™é‡‘"],
                "tertiary": ["åˆå¿ƒè€…", "å§‹ã‚æ–¹", "ãŠã™ã™ã‚", "é¸ã³æ–¹"]
            },
            "sleep_furniture": {
                "primary": ["ç¡çœ ", "ãƒ™ãƒƒãƒ‰", "ãƒãƒƒãƒˆãƒ¬ã‚¹", "å¸ƒå›£"],
                "secondary": ["é¸ã³æ–¹", "ãŠã™ã™ã‚", "å¯¾ç­–", "æ”¹å–„", "å¿«é©"],
                "tertiary": ["ä¸€äººæš®ã‚‰ã—", "ç¤¾ä¼šäºº", "å¥åº·", "ã‚µã‚¤ã‚º", "ç´ æ"]
            }
        }
        
        # æ—¢çŸ¥ã®ä½œæˆæ¸ˆã¿è¨˜äº‹ãƒ‡ãƒ¼ã‚¿ï¼ˆåˆæœŸãƒ‡ãƒ¼ã‚¿ï¼‰
        self.created_articles = {
            "blog": [
                {
                    "main_keyword": "Audible é›†ä¸­åŠ› èª­è§£åŠ›",
                    "related_keywords": ["ã‚ªãƒ¼ãƒ‡ã‚£ãƒ–ãƒ«", "è´ãèª­æ›¸", "èª­æ›¸è‹¦æ‰‹", "åŠ¹æœ", "æ–¹æ³•"],
                    "target_audience": "èª­æ›¸è‹¦æ‰‹ãªäºº",
                    "angle": "é›†ä¸­åŠ›ãƒ»èª­è§£åŠ›å‘ä¸ŠåŠ¹æœ",
                    "created_date": "2025-08-11"
                }
            ],
            "writing_project": [
                {
                    "main_keyword": "ãƒ™ãƒƒãƒ‰ãƒ•ãƒ¬ãƒ¼ãƒ  é¸ã³æ–¹",
                    "related_keywords": ["ã‚µã‚¤ã‚º", "ç´ æ", "ä¾¡æ ¼", "ãŠã™ã™ã‚"],
                    "target_audience": "ä¸€äººæš®ã‚‰ã—é–‹å§‹äºˆå®šã®ç¤¾ä¼šäºº",
                    "angle": "å¤±æ•—ã—ãªã„é¸ã³æ–¹ã‚¬ã‚¤ãƒ‰",
                    "created_date": "2025-08-12"
                },
                {
                    "main_keyword": "å¸ƒå›£ ãƒ›ã‚³ãƒªå¯¾ç­–",
                    "related_keywords": ["æƒé™¤", "ã‚¢ãƒ¬ãƒ«ã‚®ãƒ¼", "å¯¾ç­–", "æ–¹æ³•"],
                    "target_audience": "ã‚¢ãƒ¬ãƒ«ã‚®ãƒ¼ã§å›°ã£ã¦ã„ã‚‹äºº",
                    "angle": "åŠ¹æœçš„ãªãƒ›ã‚³ãƒªå¯¾ç­–æ–¹æ³•",
                    "created_date": "2025-08-11"
                },
                {
                    "main_keyword": "ç¤¾ä¼šäºº ç¡çœ æ™‚é–“",
                    "related_keywords": ["å¿™ã—ã„", "æ™‚é–“ç®¡ç†", "å¥åº·", "æ”¹å–„"],
                    "target_audience": "å¿™ã—ã„ç¤¾ä¼šäºº",
                    "angle": "åŠ¹ç‡çš„ãªç¡çœ æ™‚é–“ç¢ºä¿æ–¹æ³•",
                    "created_date": "2025-08-11"
                }
            ]
        }
    
    def extract_keywords_from_request(self, user_request: str) -> Dict[str, Any]:
        """ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‹ã‚‰ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’è‡ªå‹•æŠ½å‡º"""
        
        # åŸºæœ¬çš„ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡º
        extracted_keywords = {
            "main_keywords": [],
            "category": "unknown",
            "detected_keywords": [],
            "inferred_intent": ""
        }
        
        request_lower = user_request.lower()
        
        # ã‚«ãƒ†ã‚´ãƒªåˆ¤å®šã¨ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡º
        for category, keywords in self.keyword_categories.items():
            category_score = 0
            found_keywords = []
            
            # Primary keywordsãƒã‚§ãƒƒã‚¯
            for keyword in keywords["primary"]:
                if keyword.lower() in request_lower or keyword in user_request:
                    category_score += 3
                    found_keywords.append(keyword)
                    if keyword not in extracted_keywords["main_keywords"]:
                        extracted_keywords["main_keywords"].append(keyword)
            
            # Secondary keywordsãƒã‚§ãƒƒã‚¯
            for keyword in keywords["secondary"]:
                if keyword.lower() in request_lower or keyword in user_request:
                    category_score += 2
                    found_keywords.append(keyword)
            
            # Tertiary keywordsãƒã‚§ãƒƒã‚¯
            for keyword in keywords["tertiary"]:
                if keyword.lower() in request_lower or keyword in user_request:
                    category_score += 1
                    found_keywords.append(keyword)
            
            if category_score > 0:
                extracted_keywords["detected_keywords"].extend(found_keywords)
                if category_score > 3 and extracted_keywords["category"] == "unknown":
                    extracted_keywords["category"] = category
        
        # Intentæ¨å®š
        if "è¨˜äº‹" in user_request and "ä½œæˆ" in user_request:
            extracted_keywords["inferred_intent"] = "article_creation"
        elif "è¨˜äº‹" in user_request and "æ›¸" in user_request:
            extracted_keywords["inferred_intent"] = "article_creation"
        
        return extracted_keywords
    
    def search_relevant_archives(self, keywords: Dict[str, Any]) -> List[Dict]:
        """ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã«åŸºã¥ãé–¢é€£ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–è³‡æ–™ã®è‡ªå‹•æ¤œç´¢"""
        
        relevant_materials = []
        
        try:
            # NotebookLMè³‡æ–™ãƒ•ã‚©ãƒ«ãƒ€ã‚’ã‚¹ã‚­ãƒ£ãƒ³
            for file_path in Path(self.notebook_path).glob("*.md"):
                file_name = file_path.name
                relevance_score = 0
                matched_keywords = []
                
                # ãƒ•ã‚¡ã‚¤ãƒ«åã§ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒãƒƒãƒãƒ³ã‚°
                for keyword in keywords["main_keywords"] + keywords["detected_keywords"]:
                    if keyword.lower() in file_name.lower():
                        relevance_score += 3
                        matched_keywords.append(keyword)
                
                # ãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹ã§ã®è©³ç´°ãƒãƒƒãƒãƒ³ã‚°ï¼ˆæœ€åˆã®500æ–‡å­—ï¼‰
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        content_preview = f.read(500)
                        
                        for keyword in keywords["detected_keywords"]:
                            if keyword in content_preview:
                                relevance_score += 1
                                if keyword not in matched_keywords:
                                    matched_keywords.append(keyword)
                
                except Exception as e:
                    print(f"âš ï¸ ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿è­¦å‘Š ({file_name}): {e}")
                
                # é–¢é€£æ€§ãŒã‚ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ã‚’çµæœã«è¿½åŠ 
                if relevance_score > 0:
                    relevant_materials.append({
                        "file_name": file_name,
                        "file_path": str(file_path),
                        "relevance_score": relevance_score,
                        "matched_keywords": matched_keywords,
                        "category": keywords["category"]
                    })
            
            # é–¢é€£åº¦é †ã§ã‚½ãƒ¼ãƒˆ
            relevant_materials.sort(key=lambda x: x["relevance_score"], reverse=True)
            
        except Exception as e:
            print(f"âŒ ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–æ¤œç´¢ã‚¨ãƒ©ãƒ¼: {e}")
        
        return relevant_materials
    
    def check_duplication_risk(self, keywords: Dict[str, Any]) -> Dict[str, Any]:
        """è¨˜äº‹é‡è¤‡ãƒªã‚¹ã‚¯ã®è‡ªå‹•ãƒã‚§ãƒƒã‚¯"""
        
        duplication_result = {
            "status": "SAFE",
            "risk_level": 0,
            "conflicting_articles": [],
            "suggestions": []
        }
        
        try:
            new_main_keywords = set(keywords["main_keywords"])
            new_detected = set(keywords["detected_keywords"])
            
            # æ—¢å­˜è¨˜äº‹ã¨ã®æ¯”è¼ƒ
            all_articles = self.created_articles["blog"] + self.created_articles["writing_project"]
            
            for article in all_articles:
                existing_main = set([article["main_keyword"]])
                existing_related = set(article["related_keywords"])
                
                # ãƒ¡ã‚¤ãƒ³ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰å®Œå…¨ä¸€è‡´ãƒã‚§ãƒƒã‚¯
                main_overlap = len(new_main_keywords.intersection(existing_main))
                if main_overlap > 0:
                    overlap_ratio = main_overlap / len(new_main_keywords) if new_main_keywords else 0
                    if overlap_ratio > 0.8:
                        duplication_result["status"] = "HIGH_RISK"
                        duplication_result["risk_level"] = 9
                        duplication_result["conflicting_articles"].append({
                            "article": article["main_keyword"],
                            "overlap_type": "main_keyword_exact_match",
                            "created_date": article["created_date"]
                        })
                
                # é–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰é¡ä¼¼åº¦ãƒã‚§ãƒƒã‚¯
                related_overlap = len(new_detected.intersection(existing_related))
                if related_overlap > 2:
                    similarity_ratio = related_overlap / len(new_detected) if new_detected else 0
                    if similarity_ratio > 0.6:
                        current_risk = min(8, int(similarity_ratio * 10))
                        if current_risk > duplication_result["risk_level"]:
                            duplication_result["risk_level"] = current_risk
                            duplication_result["status"] = "MODERATE_RISK" if current_risk < 7 else "HIGH_RISK"
                            duplication_result["conflicting_articles"].append({
                                "article": article["main_keyword"],
                                "overlap_type": "related_keywords_similar",
                                "similarity_ratio": similarity_ratio,
                                "created_date": article["created_date"]
                            })
            
            # å·®åˆ¥åŒ–ææ¡ˆç”Ÿæˆ
            if duplication_result["risk_level"] > 3:
                duplication_result["suggestions"] = self._generate_differentiation_suggestions(
                    keywords, duplication_result["conflicting_articles"]
                )
        
        except Exception as e:
            print(f"âŒ é‡è¤‡ãƒã‚§ãƒƒã‚¯ã‚¨ãƒ©ãƒ¼: {e}")
        
        return duplication_result
    
    def _generate_differentiation_suggestions(self, keywords: Dict, conflicts: List[Dict]) -> List[str]:
        """å·®åˆ¥åŒ–ææ¡ˆã®è‡ªå‹•ç”Ÿæˆ"""
        
        suggestions = []
        
        # åŸºæœ¬çš„ãªå·®åˆ¥åŒ–æˆ¦ç•¥
        base_strategies = [
            "ã‚¿ãƒ¼ã‚²ãƒƒãƒˆèª­è€…å±¤ã®å¤‰æ›´ï¼ˆåˆå¿ƒè€…â†’ä¸Šç´šè€…ã€å­¦ç”Ÿâ†’ç¤¾ä¼šäººãªã©ï¼‰",
            "è¨˜äº‹ã®æ·±ã•ãƒ¬ãƒ™ãƒ«å¤‰æ›´ï¼ˆå…¥é–€ç·¨â†’å®Ÿè·µç·¨â†’å¿œç”¨ç·¨ï¼‰",
            "ã‚¢ãƒ—ãƒ­ãƒ¼ãƒè§’åº¦å¤‰æ›´ï¼ˆæ–¹æ³•è«–â†’åŠ¹æœæ¤œè¨¼â†’æ¯”è¼ƒåˆ†æï¼‰",
            "å…·ä½“çš„ç”¨é€”ãƒ»ã‚·ãƒãƒ¥ã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ç‰¹åŒ–",
            "å¤±æ•—ä¾‹ãƒ»æ³¨æ„ç‚¹ã«ãƒ•ã‚©ãƒ¼ã‚«ã‚¹ã—ãŸé€†èª¬çš„ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ"
        ]
        
        # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹ã®ç‰¹åŒ–ææ¡ˆ
        if keywords["category"] == "audible":
            suggestions.extend([
                "ç‰¹å®šãƒ‡ãƒã‚¤ã‚¹ç‰¹åŒ–ï¼ˆã‚¹ãƒãƒ›â†’Alexaâ†’Car Playï¼‰",
                "ç‰¹å®šã‚¸ãƒ£ãƒ³ãƒ«ç‰¹åŒ–ï¼ˆãƒ“ã‚¸ãƒã‚¹æ›¸â†’å°èª¬â†’å­¦ç¿’æ›¸ï¼‰",
                "åˆ©ç”¨ã‚·ãƒ¼ãƒ³ç‰¹åŒ–ï¼ˆé€šå‹¤â†’é‹å‹•â†’å°±å¯å‰ï¼‰"
            ])
        elif keywords["category"] == "audiobook":
            suggestions.extend([
                "ã‚µãƒ¼ãƒ“ã‚¹æ¯”è¼ƒç‰¹åŒ–ï¼ˆAudible vs audiobook.jpï¼‰",
                "æ–™é‡‘ãƒ»ã‚³ã‚¹ãƒ‘åˆ†æç‰¹åŒ–",
                "æ©Ÿèƒ½ãƒ»ä½¿ã„ã‚„ã™ã•æ¯”è¼ƒç‰¹åŒ–"
            ])
        elif keywords["category"] == "sleep_furniture":
            suggestions.extend([
                "äºˆç®—å¸¯åˆ¥ç‰¹åŒ–ï¼ˆ1ä¸‡å††ä»¥ä¸‹â†’3ä¸‡å††ä»¥ä¸‹â†’é«˜ç´šå“ï¼‰",
                "ä½ç’°å¢ƒåˆ¥ç‰¹åŒ–ï¼ˆä¸€äººæš®ã‚‰ã—â†’ãƒ•ã‚¡ãƒŸãƒªãƒ¼â†’é«˜é½¢è€…ï¼‰",
                "ä½“å‹ãƒ»ä½“è³ªåˆ¥ç‰¹åŒ–ï¼ˆèº«é•·åˆ¥â†’ä½“é‡åˆ¥â†’è…°ç—›å¯¾ç­–ï¼‰"
            ])
        
        # ç«¶åˆè¨˜äº‹ã¨ã®å·®åˆ¥åŒ–ãƒã‚¤ãƒ³ãƒˆ
        if conflicts:
            suggestions.extend([
                f"æ—¢å­˜è¨˜äº‹ï¼ˆ{conflicts[0]['article']}ï¼‰ã¨ã®æ˜ç¢ºãªå·®åˆ¥åŒ–ãƒã‚¤ãƒ³ãƒˆè¨­å®š",
                "æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ãƒ»ã‚¨ãƒ“ãƒ‡ãƒ³ã‚¹ã®è¿½åŠ ",
                "ã‚ˆã‚Šå®Ÿè·µçš„ãƒ»å…·ä½“çš„ãªå†…å®¹ã«ç‰¹åŒ–"
            ])
        
        suggestions.extend(base_strategies)
        
        return suggestions[:8]  # æœ€å¤§8å€‹ã¾ã§
    
    def extract_quality_elements(self, relevant_materials: List[Dict]) -> Dict[str, Any]:
        """é–¢é€£è³‡æ–™ã‹ã‚‰å“è³ªå‘ä¸Šè¦ç´ ã‚’è‡ªå‹•æŠ½å‡º"""
        
        quality_elements = {
            "proven_structures": [],
            "effective_phrases": [],
            "reliable_data_sources": [],
            "successful_keywords": [],
            "engagement_patterns": []
        }
        
        try:
            for material in relevant_materials[:3]:  # ä¸Šä½3ã¤ã®é–¢é€£è³‡æ–™ã‚’åˆ†æ
                file_path = material["file_path"]
                
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                
                # æ§‹é€ ãƒ‘ã‚¿ãƒ¼ãƒ³æŠ½å‡ºï¼ˆè¦‹å‡ºã—æ§‹é€ ï¼‰
                headings = re.findall(r'^(#{1,6})\s+(.+)$', content, re.MULTILINE)
                if headings:
                    structure_pattern = [f"H{len(h[0])}:{h[1][:30]}..." for h in headings[:5]]
                    quality_elements["proven_structures"].append({
                        "source": material["file_name"],
                        "structure": structure_pattern
                    })
                
                # åŠ¹æœçš„ãªãƒ•ãƒ¬ãƒ¼ã‚ºæŠ½å‡ºï¼ˆå¼·èª¿è¡¨ç¾ï¼‰
                effective_phrases = re.findall(r'\*\*(.+?)\*\*', content)
                if effective_phrases:
                    quality_elements["effective_phrases"].extend(effective_phrases[:5])
                
                # ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹æŠ½å‡ºï¼ˆå…·ä½“çš„ãªæ•°å€¤ãƒ»ç ”ç©¶çµæœï¼‰
                data_patterns = re.findall(r'(\d+%|\d+äºº|\d+å€|\d+å††|\d+å†Š)', content)
                if data_patterns:
                    quality_elements["reliable_data_sources"].extend(data_patterns[:5])
                
                # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ä½¿ç”¨ãƒ‘ã‚¿ãƒ¼ãƒ³æŠ½å‡º
                for keyword in material["matched_keywords"]:
                    if keyword not in quality_elements["successful_keywords"]:
                        quality_elements["successful_keywords"].append(keyword)
        
        except Exception as e:
            print(f"âŒ å“è³ªè¦ç´ æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
        
        return quality_elements
    
    def generate_enhanced_suggestions(self, keywords: Dict, relevant_materials: List[Dict], 
                                    duplication_check: Dict, quality_elements: Dict) -> Dict[str, Any]:
        """çµ±åˆæƒ…å ±ã«åŸºã¥ãè¨˜äº‹ä½œæˆææ¡ˆã®ç”Ÿæˆ"""
        
        suggestions = {
            "recommended_approach": "",
            "content_structure_suggestions": [],
            "keyword_optimization": [],
            "quality_enhancement_tips": [],
            "differentiation_strategy": "",
            "risk_mitigation": []
        }
        
        try:
            # æ¨å¥¨ã‚¢ãƒ—ãƒ­ãƒ¼ãƒæ±ºå®š
            if duplication_check["status"] == "SAFE":
                suggestions["recommended_approach"] = "é€šå¸¸ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã§é«˜å“è³ªè¨˜äº‹ä½œæˆ"
            elif duplication_check["status"] == "MODERATE_RISK":
                suggestions["recommended_approach"] = "å·®åˆ¥åŒ–ãƒã‚¤ãƒ³ãƒˆã‚’æ˜ç¢ºã«ã—ãŸç‰¹åŒ–å‹ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ"
            else:
                suggestions["recommended_approach"] = "å¤§å¹…ãªè§’åº¦å¤‰æ›´ã¾ãŸã¯ä¼ç”»è¦‹ç›´ã—æ¨å¥¨"
            
            # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„æ§‹é€ ææ¡ˆï¼ˆé–¢é€£è³‡æ–™ã®æˆåŠŸãƒ‘ã‚¿ãƒ¼ãƒ³æ´»ç”¨ï¼‰
            if quality_elements["proven_structures"]:
                suggestions["content_structure_suggestions"] = [
                    f"å‚è€ƒæ§‹é€ : {struct['source']} - {struct['structure']}"
                    for struct in quality_elements["proven_structures"][:2]
                ]
            
            # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æœ€é©åŒ–ææ¡ˆ
            successful_keywords = quality_elements["successful_keywords"][:5]
            suggestions["keyword_optimization"] = [
                f"é«˜åŠ¹æœã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ´»ç”¨: {', '.join(successful_keywords)}",
                f"ã‚«ãƒ†ã‚´ãƒªç‰¹åŒ–: {keywords['category']}ç³»ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰å¼·åŒ–æ¨å¥¨"
            ]
            
            # å“è³ªå‘ä¸Šã®ãƒ’ãƒ³ãƒˆ
            if quality_elements["effective_phrases"]:
                suggestions["quality_enhancement_tips"] = [
                    f"åŠ¹æœçš„ãªè¡¨ç¾æ´»ç”¨: {phrase}" for phrase in quality_elements["effective_phrases"][:3]
                ]
            
            if quality_elements["reliable_data_sources"]:
                suggestions["quality_enhancement_tips"].append(
                    f"å…·ä½“çš„ãƒ‡ãƒ¼ã‚¿æ´»ç”¨: {', '.join(quality_elements['reliable_data_sources'][:3])}"
                )
            
            # å·®åˆ¥åŒ–æˆ¦ç•¥
            if duplication_check["suggestions"]:
                suggestions["differentiation_strategy"] = duplication_check["suggestions"][0]
            
            # ãƒªã‚¹ã‚¯ç·©å’Œç­–
            if duplication_check["status"] != "SAFE":
                suggestions["risk_mitigation"] = [
                    "æ—¢å­˜è¨˜äº‹ã¨ã®æ˜ç¢ºãªå·®åˆ¥åŒ–ãƒã‚¤ãƒ³ãƒˆè¨­å®š",
                    "ã‚¿ãƒ¼ã‚²ãƒƒãƒˆèª­è€…å±¤ã®ç‰¹åŒ–",
                    "æ–°ã—ã„æƒ…å ±ãƒ»è§’åº¦ã®è¿½åŠ "
                ]
        
        except Exception as e:
            print(f"âŒ ææ¡ˆç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
        
        return suggestions
    
    def auto_archive_utilization_workflow(self, user_request: str) -> Dict[str, Any]:
        """çµ±åˆè‡ªå‹•æ´»ç”¨ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ - ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
        
        print("ğŸ” çµ±åˆè³‡æ–™ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–è‡ªå‹•æ´»ç”¨ã‚·ã‚¹ãƒ†ãƒ èµ·å‹•...")
        
        workflow_result = {
            "keywords": {},
            "relevant_materials": [],
            "duplication_check": {},
            "quality_elements": {},
            "suggestions": {},
            "workflow_status": "success"
        }
        
        try:
            # Step 1: ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰è‡ªå‹•æŠ½å‡º
            print("âš™ï¸ Step 1: ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰è‡ªå‹•æŠ½å‡º...")
            workflow_result["keywords"] = self.extract_keywords_from_request(user_request)
            print(f"âœ… æŠ½å‡ºå®Œäº†: ã‚«ãƒ†ã‚´ãƒª={workflow_result['keywords']['category']}, "
                  f"ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ•°={len(workflow_result['keywords']['detected_keywords'])}")
            
            # Step 2: é–¢é€£è³‡æ–™è‡ªå‹•æ¤œç´¢
            print("âš™ï¸ Step 2: é–¢é€£è³‡æ–™è‡ªå‹•æ¤œç´¢...")
            workflow_result["relevant_materials"] = self.search_relevant_archives(workflow_result["keywords"])
            print(f"âœ… æ¤œç´¢å®Œäº†: é–¢é€£è³‡æ–™{len(workflow_result['relevant_materials'])}ä»¶ç™ºè¦‹")
            
            # Step 3: é‡è¤‡ãƒªã‚¹ã‚¯è‡ªå‹•ãƒã‚§ãƒƒã‚¯
            print("âš™ï¸ Step 3: é‡è¤‡ãƒªã‚¹ã‚¯è‡ªå‹•ãƒã‚§ãƒƒã‚¯...")
            workflow_result["duplication_check"] = self.check_duplication_risk(workflow_result["keywords"])
            print(f"âœ… ãƒã‚§ãƒƒã‚¯å®Œäº†: ãƒªã‚¹ã‚¯ãƒ¬ãƒ™ãƒ«={workflow_result['duplication_check']['status']}")
            
            # Step 4: å“è³ªè¦ç´ è‡ªå‹•æŠ½å‡º
            print("âš™ï¸ Step 4: å“è³ªå‘ä¸Šè¦ç´ è‡ªå‹•æŠ½å‡º...")
            workflow_result["quality_elements"] = self.extract_quality_elements(workflow_result["relevant_materials"])
            print(f"âœ… æŠ½å‡ºå®Œäº†: å“è³ªè¦ç´ {len(workflow_result['quality_elements']['successful_keywords'])}å€‹ç‰¹å®š")
            
            # Step 5: çµ±åˆææ¡ˆç”Ÿæˆ
            print("âš™ï¸ Step 5: çµ±åˆææ¡ˆè‡ªå‹•ç”Ÿæˆ...")
            workflow_result["suggestions"] = self.generate_enhanced_suggestions(
                workflow_result["keywords"],
                workflow_result["relevant_materials"],
                workflow_result["duplication_check"],
                workflow_result["quality_elements"]
            )
            print("âœ… ææ¡ˆç”Ÿæˆå®Œäº†")
            
            print("ğŸ‰ çµ±åˆè³‡æ–™ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–æ´»ç”¨ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼å®Œäº†ï¼")
            
        except Exception as e:
            print(f"âŒ ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚¨ãƒ©ãƒ¼: {e}")
            workflow_result["workflow_status"] = "error"
            workflow_result["error_message"] = str(e)
        
        return workflow_result
    
    def generate_workflow_report(self, workflow_result: Dict[str, Any]) -> str:
        """ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼çµæœãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        
        if workflow_result["workflow_status"] == "error":
            return f"âŒ ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚¨ãƒ©ãƒ¼: {workflow_result.get('error_message', 'ä¸æ˜ãªã‚¨ãƒ©ãƒ¼')}"
        
        report = f"""
ğŸ“Š çµ±åˆè³‡æ–™ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–æ´»ç”¨ãƒ¬ãƒãƒ¼ãƒˆ
{'='*50}

ğŸ” ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ†æçµæœ:
- æ¤œå‡ºã‚«ãƒ†ã‚´ãƒª: {workflow_result['keywords']['category']}
- ãƒ¡ã‚¤ãƒ³ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {', '.join(workflow_result['keywords']['main_keywords'])}
- é–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {len(workflow_result['keywords']['detected_keywords'])}å€‹æ¤œå‡º

ğŸ“š é–¢é€£è³‡æ–™ç™ºè¦‹çŠ¶æ³:
- ç™ºè¦‹è³‡æ–™æ•°: {len(workflow_result['relevant_materials'])}ä»¶
- é«˜é–¢é€£åº¦è³‡æ–™: {len([m for m in workflow_result['relevant_materials'] if m['relevance_score'] > 5])}ä»¶

ğŸš« é‡è¤‡ãƒªã‚¹ã‚¯åˆ†æ:
- ãƒªã‚¹ã‚¯ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹: {workflow_result['duplication_check']['status']}
- ãƒªã‚¹ã‚¯ãƒ¬ãƒ™ãƒ«: {workflow_result['duplication_check']['risk_level']}/10
- ç«¶åˆè¨˜äº‹: {len(workflow_result['duplication_check']['conflicting_articles'])}ä»¶

ğŸ’¡ å“è³ªå‘ä¸Šè¦ç´ :
- æˆåŠŸå®Ÿç¸¾ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {len(workflow_result['quality_elements']['successful_keywords'])}å€‹
- åŠ¹æœçš„ãƒ•ãƒ¬ãƒ¼ã‚º: {len(workflow_result['quality_elements']['effective_phrases'])}å€‹
- å‚è€ƒæ§‹é€ ãƒ‘ã‚¿ãƒ¼ãƒ³: {len(workflow_result['quality_elements']['proven_structures'])}å€‹

ğŸ¯ æ¨å¥¨ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ:
{workflow_result['suggestions']['recommended_approach']}

{'='*50}
âœ… ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–æ´»ç”¨ã‚·ã‚¹ãƒ†ãƒ åˆ†æå®Œäº†
"""
        
        return report


def main():
    """ãƒ†ã‚¹ãƒˆå®Ÿè¡Œç”¨ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    system = ArchiveUtilizationSystem()
    
    # ãƒ†ã‚¹ãƒˆç”¨ãƒªã‚¯ã‚¨ã‚¹ãƒˆ
    test_requests = [
        "Audibleã®ä½¿ã„æ–¹ã«ã¤ã„ã¦è¨˜äº‹ã‚’ä½œæˆã—ãŸã„",
        "ã‚ªãƒ¼ãƒ‡ã‚£ã‚ªãƒ–ãƒƒã‚¯ã®åŠ¹æœã«ã¤ã„ã¦è©³ã—ãæ›¸ããŸã„",
        "ãƒ™ãƒƒãƒ‰ã®é¸ã³æ–¹ã«ã¤ã„ã¦è¨˜äº‹ã‚’æ›¸ã„ã¦"
    ]
    
    for request in test_requests:
        print(f"\nğŸ§ª ãƒ†ã‚¹ãƒˆ: {request}")
        result = system.auto_archive_utilization_workflow(request)
        report = system.generate_workflow_report(result)
        print(report)


if __name__ == "__main__":
    main()