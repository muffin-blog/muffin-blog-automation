#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
CLIè‡ªå‹•è¨˜äº‹ä½œæˆã‚·ã‚¹ãƒ†ãƒ  - ãƒ©ã‚¤ãƒ†ã‚£ãƒ³ã‚°æ¡ˆä»¶ï¼ˆãƒŸãƒãƒ«ãƒ´ã‚¹ãƒªãƒ¼ãƒ—ï¼‰
ãƒ†ã‚­ã‚¹ãƒˆè³‡æ–™ã‚’FINALç‰ˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã§é«˜å“è³ªè¨˜äº‹ã«å¤‰æ›

çµ±åˆã‚·ã‚¹ãƒ†ãƒ :
- FINALç‰ˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆå®Œå…¨æº–æ‹ 
- WebSearchèª­è€…ãƒ‹ãƒ¼ã‚ºåˆ†æ
- ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒªã‚ªé€£æºã‚·ã‚¹ãƒ†ãƒ å¯¾å¿œ
"""

import os
import re
import json
import sys
from datetime import datetime
from typing import Dict, List, Optional, Tuple
from pathlib import Path

# çµ±åˆã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ã‚·ã‚¹ãƒ†ãƒ  import
sys.path.append("/Users/satoumasamitsu/Desktop/osigoto/çµ±åˆç®¡ç†ã‚·ã‚¹ãƒ†ãƒ /è³‡æ–™ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–/")
try:
    from archive_utilization_system import ArchiveUtilizationSystem
except ImportError:
    print("âš ï¸ ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ã‚·ã‚¹ãƒ†ãƒ ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚åŸºæœ¬æ©Ÿèƒ½ã®ã¿ã§å‹•ä½œã—ã¾ã™ã€‚")
    ArchiveUtilizationSystem = None

class CLIAutoWritingSystem:
    """CLIè‡ªå‹•è¨˜äº‹ä½œæˆã‚·ã‚¹ãƒ†ãƒ  - ãƒ©ã‚¤ãƒ†ã‚£ãƒ³ã‚°æ¡ˆä»¶å´"""
    
    def __init__(self):
        """åˆæœŸåŒ–"""
        self.base_path = "/Users/satoumasamitsu/Desktop/osigoto/ãƒ©ã‚¤ãƒ†ã‚£ãƒ³ã‚°æ¡ˆä»¶/ãƒŸãƒãƒ«ãƒ´ã‚¹ãƒªãƒ¼ãƒ—/"
        self.template_path = os.path.join(self.base_path, "ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ/è¨˜äº‹ä½œæˆå®Œå…¨ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ_FINAL.md")
        self.work_in_progress_path = os.path.join(self.base_path, "è¨˜äº‹/3_ä½œæˆä¸­/")
        self.completed_path = os.path.join(self.base_path, "è¨˜äº‹/2_å®Œæˆè¨˜äº‹/")
        self.reference_articles_path = os.path.join(self.base_path, "è¨˜äº‹/2_å®Œæˆè¨˜äº‹/è¿½åŠ è¨˜äº‹/")
        
        # å‚ç…§è¨˜äº‹ï¼ˆ2025å¹´8æœˆ11æ—¥è¨˜äº‹ï¼‰
        self.reference_articles = [
            "2025.08.11.å¸ƒå›£.ãƒ›ã‚³ãƒªå¯¾ç­–.md",
            "2025.08.11.ç¤¾ä¼šäºº.ç¡çœ æ™‚é–“.md"
        ]
        
        # + Î± çµ±åˆã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
        self.archive_system = None
        if ArchiveUtilizationSystem:
            try:
                self.archive_system = ArchiveUtilizationSystem()
                print("âœ… çµ±åˆã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ã‚·ã‚¹ãƒ†ãƒ é€£æºå®Œäº†")
            except Exception as e:
                print(f"âš ï¸ ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–è­¦å‘Š: {e}")
        else:
            print("â„¹ï¸ åŸºæœ¬æ©Ÿèƒ½ã®ã¿ã§å‹•ä½œä¸­ï¼ˆã‚¢ãƒ¼ã‚«ã‚¤ãƒ–æ©Ÿèƒ½ç„¡åŠ¹ï¼‰")
    
    def load_final_template(self) -> str:
        """FINALç‰ˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’èª­ã¿è¾¼ã¿"""
        try:
            with open(self.template_path, 'r', encoding='utf-8') as f:
                return f.read()
        except Exception as e:
            print(f"âŒ FINALç‰ˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            return ""
    
    def load_reference_articles(self) -> Dict[str, str]:
        """å‚ç…§è¨˜äº‹ã‚’èª­ã¿è¾¼ã¿ï¼ˆPhase 0å¯¾å¿œï¼‰"""
        references = {}
        
        for article_name in self.reference_articles:
            try:
                article_path = os.path.join(self.reference_articles_path, article_name)
                if os.path.exists(article_path):
                    with open(article_path, 'r', encoding='utf-8') as f:
                        references[article_name] = f.read()
                    print(f"âœ… å‚ç…§è¨˜äº‹èª­ã¿è¾¼ã¿: {article_name}")
                else:
                    print(f"âš ï¸ å‚ç…§è¨˜äº‹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {article_name}")
            except Exception as e:
                print(f"âŒ å‚ç…§è¨˜äº‹èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼ ({article_name}): {e}")
        
        return references
    
    def parse_input_data(self, input_text: str) -> Dict:
        """ãƒ†ã‚­ã‚¹ãƒˆè³‡æ–™ã‚’è§£æ"""
        try:
            # åŸºæœ¬çš„ãªãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒãƒ³ã‚°
            data = {
                "project": "",
                "main_keyword": "",
                "related_keywords": [],
                "competitor_analysis": "",
                "target_audience": "",
                "other_requirements": ""
            }
            
            lines = input_text.strip().split('\n')
            current_section = None
            
            for line in lines:
                line = line.strip()
                
                if line.startswith('ã€æ¡ˆä»¶ã€‘') or line.startswith('ã€ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã€‘'):
                    data["project"] = line.split('ã€‘')[-1].strip().replace(':', '').replace('ï¼š', '')
                
                elif line.startswith('ã€ãƒ¡ã‚¤ãƒ³ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã€‘'):
                    data["main_keyword"] = line.split('ã€‘')[-1].strip().replace(':', '').replace('ï¼š', '')
                
                elif line.startswith('ã€é–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã€‘'):
                    keywords_str = line.split('ã€‘')[-1].strip().replace(':', '').replace('ï¼š', '')
                    data["related_keywords"] = [k.strip() for k in keywords_str.split(',') if k.strip()]
                
                elif line.startswith('ã€ä¸Šä½è¨˜äº‹åˆ†æã€‘') or line.startswith('ã€ç«¶åˆåˆ†æã€‘'):
                    current_section = "competitor"
                    data["competitor_analysis"] = ""
                
                elif line.startswith('ã€ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã€‘') or line.startswith('ã€èª­è€…ã€‘'):
                    data["target_audience"] = line.split('ã€‘')[-1].strip().replace(':', '').replace('ï¼š', '')
                
                elif line.startswith('ã€ãã®ä»–ã€‘') or line.startswith('ã€è¦æœ›ã€‘'):
                    data["other_requirements"] = line.split('ã€‘')[-1].strip().replace(':', '').replace('ï¼š', '')
                
                elif current_section == "competitor" and line:
                    data["competitor_analysis"] += line + "\\n"
            
            return data
            
        except Exception as e:
            print(f"âŒ å…¥åŠ›ãƒ‡ãƒ¼ã‚¿è§£æã‚¨ãƒ©ãƒ¼: {e}")
            return {}
    
    def analyze_reader_needs(self, main_keyword: str, related_keywords: List[str]) -> Dict:
        """èª­è€…ãƒ‹ãƒ¼ã‚ºåˆ†æï¼ˆWebSearchä½¿ç”¨ï¼‰"""
        print(f"ğŸ” èª­è€…ãƒ‹ãƒ¼ã‚ºåˆ†æä¸­: {main_keyword}")
        
        # WebSearchç”¨ã‚¯ã‚¨ãƒªç”Ÿæˆ
        search_queries = [
            f"{main_keyword} æ‚©ã¿",
            f"{main_keyword} ä¸å®‰",
            f"{main_keyword} ç–‘å•",
            f"{main_keyword} é¸ã³æ–¹",
            f"{main_keyword} å¤±æ•—"
        ]
        
        # é–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‹ã‚‰ã‚‚æ¤œç´¢ã‚¯ã‚¨ãƒªç”Ÿæˆ
        for keyword in related_keywords[:3]:  # ä¸Šä½3ã¤ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
            search_queries.append(f"{keyword} {main_keyword}")
        
        # å®Ÿéš›ã®WebSearchå®Ÿè¡Œã¯å¾Œã§å®Ÿè£…
        # ç¾åœ¨ã¯åˆ†æãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ãƒ™ãƒ¼ã‚¹ã§ä»®ç”Ÿæˆ
        reader_analysis = {
            "primary_concerns": self._generate_primary_concerns(main_keyword, related_keywords),
            "search_intent": self._analyze_search_intent(main_keyword),
            "pain_points": self._identify_pain_points(main_keyword, related_keywords),
            "desired_outcomes": self._identify_desired_outcomes(main_keyword),
            "knowledge_level": self._assess_knowledge_level(main_keyword)
        }
        
        print("âœ… èª­è€…ãƒ‹ãƒ¼ã‚ºåˆ†æå®Œäº†")
        return reader_analysis
    
    def _generate_primary_concerns(self, main_keyword: str, related_keywords: List[str]) -> List[str]:
        """ä¸»è¦ãªæ‚©ã¿ãƒ»ä¸å®‰ã‚’ç”Ÿæˆ"""
        concerns = []
        
        # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹ã®æ‚©ã¿ç”Ÿæˆ
        if "é¸ã³æ–¹" in main_keyword or "é¸ã³æ–¹" in ' '.join(related_keywords):
            concerns.extend([
                "ã©ã‚Œã‚’é¸ã‚“ã§ã„ã„ã‹ã‚ã‹ã‚‰ãªã„",
                "å¤±æ•—ã—ãŸããªã„",
                "è‡ªåˆ†ã«åˆã†ã‚‚ã®ãŒã‚ã‹ã‚‰ãªã„"
            ])
        
        if "ã‚µã‚¤ã‚º" in ' '.join(related_keywords):
            concerns.append("ã‚µã‚¤ã‚ºé¸ã³ã§å¤±æ•—ã—ãã†")
        
        if "äºˆç®—" in ' '.join(related_keywords) or "ä¾¡æ ¼" in ' '.join(related_keywords):
            concerns.extend([
                "äºˆç®—å†…ã§è‰¯ã„ã‚‚ã®ãŒè¦‹ã¤ã‹ã‚‹ã‹ä¸å®‰",
                "é«˜ã„ã‚‚ã®ã¨å®‰ã„ã‚‚ã®ã®é•ã„ãŒã‚ã‹ã‚‰ãªã„"
            ])
        
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆæ‚©ã¿
        if not concerns:
            concerns = [
                "åˆã‚ã¦ã§ä½•ã‚‚ã‚ã‹ã‚‰ãªã„",
                "å¤±æ•—ã—ãŸããªã„",
                "å¾Œæ‚”ã—ãŸããªã„"
            ]
        
        return concerns[:5]  # æœ€å¤§5ã¤ã¾ã§
    
    def _analyze_search_intent(self, main_keyword: str) -> str:
        """æ¤œç´¢æ„å›³ã®åˆ†æ"""
        if "é¸ã³æ–¹" in main_keyword:
            return "å•†å“é¸æŠãƒ»æ¯”è¼ƒæ¤œè¨"
        elif "ãŠã™ã™ã‚" in main_keyword:
            return "æ¨å¥¨å•†å“ãƒ»ãƒ©ãƒ³ã‚­ãƒ³ã‚°æƒ…å ±"
        elif "æ–¹æ³•" in main_keyword:
            return "å…·ä½“çš„ãªæ‰‹é †ãƒ»ã‚„ã‚Šæ–¹"
        else:
            return "åŸºæœ¬çš„ãªæƒ…å ±åé›†"
    
    def _identify_pain_points(self, main_keyword: str, related_keywords: List[str]) -> List[str]:
        """ç—›ç‚¹ãƒ»èª²é¡Œã®ç‰¹å®š"""
        pain_points = []
        
        keywords_text = main_keyword + ' ' + ' '.join(related_keywords)
        
        if "ãƒ™ãƒƒãƒ‰" in keywords_text:
            pain_points.extend([
                "éƒ¨å±‹ã®ã‚µã‚¤ã‚ºã«åˆã‚ãªã„",
                "çµ„ã¿ç«‹ã¦ãŒå¤§å¤‰",
                "æ¬å…¥ã§ããªã„"
            ])
        
        if "ãƒãƒƒãƒˆãƒ¬ã‚¹" in keywords_text:
            pain_points.extend([
                "ç¡¬ã•ãŒä½“ã«åˆã‚ãªã„",
                "å¯å¿ƒåœ°ãŒæ‚ªã„",
                "è…°ç—›ã«ãªã‚‹"
            ])
        
        if "ç¡çœ " in keywords_text:
            pain_points.extend([
                "ãªã‹ãªã‹çœ ã‚Œãªã„",
                "æœèµ·ãã‚‹ã®ãŒã¤ã‚‰ã„",
                "ç–²ã‚ŒãŒå–ã‚Œãªã„"
            ])
        
        return pain_points[:4]  # æœ€å¤§4ã¤ã¾ã§
    
    def _identify_desired_outcomes(self, main_keyword: str) -> List[str]:
        """æœ›ã‚€çµæœã®ç‰¹å®š"""
        outcomes = [
            "å¤±æ•—ã›ãšã«é¸ã³ãŸã„",
            "æº€è¶³ã§ãã‚‹ã‚‚ã®ã‚’è³¼å…¥ã—ãŸã„",
            "é•·ãä½¿ãˆã‚‹ã‚‚ã®ãŒæ¬²ã—ã„",
            "ã‚³ã‚¹ãƒˆãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã®è‰¯ã„ã‚‚ã®ã‚’è¦‹ã¤ã‘ãŸã„"
        ]
        
        return outcomes
    
    def _assess_knowledge_level(self, main_keyword: str) -> str:
        """èª­è€…ã®çŸ¥è­˜ãƒ¬ãƒ™ãƒ«è©•ä¾¡"""
        if "åˆå¿ƒè€…" in main_keyword or "å§‹ã‚æ–¹" in main_keyword:
            return "åˆå¿ƒè€…ãƒ¬ãƒ™ãƒ«ï¼ˆåŸºæœ¬ã‹ã‚‰èª¬æ˜å¿…è¦ï¼‰"
        elif "é¸ã³æ–¹" in main_keyword:
            return "ä¸­ç´šè€…ãƒ¬ãƒ™ãƒ«ï¼ˆæ¯”è¼ƒæ¤œè¨æ®µéšï¼‰"
        elif "ãŠã™ã™ã‚" in main_keyword:
            return "æ±ºå®šæ®µéšï¼ˆå…·ä½“çš„ãªå•†å“æƒ…å ±ãŒå¿…è¦ï¼‰"
        else:
            return "åˆç´šï½ä¸­ç´šè€…ãƒ¬ãƒ™ãƒ«ï¼ˆä¸å¯§ãªèª¬æ˜ãŒå¿…è¦ï¼‰"
    
    def create_article_structure(self, input_data: Dict, reader_analysis: Dict, references: Dict) -> str:
        """è¨˜äº‹æ§‹é€ ãƒ»éª¨æ ¼ã‚’ä½œæˆ"""
        print("ğŸ“ è¨˜äº‹æ§‹é€ ä½œæˆä¸­...")
        
        # å‚ç…§è¨˜äº‹ã‹ã‚‰æ§‹æˆãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å­¦ç¿’
        reference_patterns = self._analyze_reference_patterns(references)
        
        # ãƒ¡ã‚¤ãƒ³ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‹ã‚‰ã‚¿ã‚¤ãƒˆãƒ«ç”Ÿæˆ
        title = self._generate_title(input_data["main_keyword"], input_data["related_keywords"])
        
        # è¦‹å‡ºã—æ§‹æˆç”Ÿæˆ
        structure = self._generate_heading_structure(
            input_data, reader_analysis, reference_patterns
        )
        
        # éª¨æ ¼ãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ
        skeleton = f"""# è¨˜äº‹éª¨æ ¼ï¼š{title}

## ğŸ“Š è¨˜äº‹æƒ…å ±
- **ãƒ¡ã‚¤ãƒ³ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰**: {input_data["main_keyword"]}
- **é–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰**: {", ".join(input_data["related_keywords"])}
- **å¯¾è±¡èª­è€…**: {reader_analysis["knowledge_level"]}
- **æ¤œç´¢æ„å›³**: {reader_analysis["search_intent"]}

## ğŸ¯ èª­è€…ãƒ‹ãƒ¼ã‚ºåˆ†æçµæœ

### ä¸»è¦ãªæ‚©ã¿ãƒ»ä¸å®‰
{chr(10).join([f"- {concern}" for concern in reader_analysis["primary_concerns"]])}

### ç—›ç‚¹ãƒ»èª²é¡Œ
{chr(10).join([f"- {pain}" for pain in reader_analysis["pain_points"]])}

### æœ›ã‚€çµæœ
{chr(10).join([f"- {outcome}" for outcome in reader_analysis["desired_outcomes"]])}

## ğŸ“ è¨˜äº‹æ§‹æˆæ¡ˆ

{structure}

## ğŸ¨ åŸ·ç­†æ–¹é‡
- FINALç‰ˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆå®Œå…¨æº–æ‹ 
- å‚ç…§è¨˜äº‹ï¼ˆ8æœˆ11æ—¥ï¼‰ã®ãƒˆãƒ¼ãƒ³ã‚’è¸è¥²
- èª­è€…ã®æ‚©ã¿ãƒ•ã‚¡ãƒ¼ã‚¹ãƒˆã§æ§‹æˆ
- å…·ä½“çš„ã§å®Ÿè·µã—ã‚„ã™ã„å†…å®¹
- ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«åŠ¹ç‡ï¼ˆ1è¦‹å‡ºã—1ç®‡æ¡æ›¸ãï¼‰éµå®ˆ

---
**ä½œæˆæ—¥æ™‚**: {datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥ %Hæ™‚%Måˆ†')}
**ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹**: éª¨æ ¼ä½œæˆå®Œäº†ãƒ»æ‰¿èªå¾…ã¡
"""
        
        print("âœ… è¨˜äº‹æ§‹é€ ä½œæˆå®Œäº†")
        return skeleton
    
    def _analyze_reference_patterns(self, references: Dict) -> Dict:
        """å‚ç…§è¨˜äº‹ã®ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ"""
        patterns = {
            "heading_style": "## ",
            "numbered_lists": True,
            "conclusion_pattern": "ã¾ã¨ã‚",
            "tone": "è¦ªã—ã¿ã‚„ã™ã„ãƒ»å®Ÿç”¨çš„"
        }
        
        # å®Ÿéš›ã®å‚ç…§è¨˜äº‹ã‹ã‚‰è¦‹å‡ºã—ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æŠ½å‡º
        for article_name, content in references.items():
            lines = content.split('\n')
            for line in lines:
                if line.startswith('## '):
                    # è¦‹å‡ºã—ãƒ‘ã‚¿ãƒ¼ãƒ³ã®åˆ†æ
                    if '5é¸' in line or '3ã¤' in line:
                        patterns["uses_numbers"] = True
                    if 'ãƒ¡ãƒªãƒƒãƒˆ' in line or 'ãƒ‡ãƒ¡ãƒªãƒƒãƒˆ' in line:
                        patterns["pros_cons"] = True
        
        return patterns
    
    def _generate_title(self, main_keyword: str, related_keywords: List[str]) -> str:
        """ã‚¿ã‚¤ãƒˆãƒ«ç”Ÿæˆï¼ˆ31æ–‡å­—å‰å¾Œï¼‰"""
        
        # é–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‹ã‚‰ã‚µãƒ–ãƒ¯ãƒ¼ãƒ‰æŠ½å‡º
        sub_words = []
        for keyword in related_keywords[:3]:
            if len(keyword) <= 4:  # çŸ­ã„ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å„ªå…ˆ
                sub_words.append(keyword)
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒ™ãƒ¼ã‚¹ã§ã‚¿ã‚¤ãƒˆãƒ«ç”Ÿæˆ
        if "é¸ã³æ–¹" in main_keyword:
            title = f"{main_keyword.replace('é¸ã³æ–¹', '')}é¸ã³ã§å¤±æ•—ã—ãªã„ï¼{main_keyword}å®Œå…¨ã‚¬ã‚¤ãƒ‰"
        elif "ãŠã™ã™ã‚" in main_keyword:
            title = f"{main_keyword}ï½œãƒ—ãƒ­ãŒå³é¸ã™ã‚‹{sub_words[0] if sub_words else 'æœ€é©'}ãªé¸ã³æ–¹"
        else:
            title = f"{main_keyword}ã®å…¨ã¦ï½œåˆå¿ƒè€…ã§ã‚‚å¤±æ•—ã—ãªã„é¸ã³æ–¹ã‚¬ã‚¤ãƒ‰"
        
        # æ–‡å­—æ•°èª¿æ•´ï¼ˆ31æ–‡å­—å‰å¾Œï¼‰
        if len(title) > 35:
            title = title[:32] + "..."
        elif len(title) < 28:
            title = title + "ã€2025å¹´ç‰ˆã€‘"
        
        return title
    
    def _generate_heading_structure(self, input_data: Dict, reader_analysis: Dict, patterns: Dict) -> str:
        """è¦‹å‡ºã—æ§‹æˆç”Ÿæˆ"""
        
        main_keyword = input_data["main_keyword"]
        related_keywords = input_data["related_keywords"]
        
        structure = f"""### H1: {self._generate_title(main_keyword, related_keywords)}

### å°å…¥æ–‡ï¼ˆèª­è€…ã®æ‚©ã¿ã«å…±æ„Ÿï¼‰
- {reader_analysis["primary_concerns"][0] if reader_analysis["primary_concerns"] else "èª­è€…ã®æ‚©ã¿"}ã«ã¤ã„ã¦å…±æ„Ÿçš„ãªå°å…¥

### H2: {main_keyword}ã‚’é¸ã¶å‰ã«çŸ¥ã£ã¦ãŠããŸã„åŸºæœ¬çŸ¥è­˜
- åŸºæœ¬çš„ãªçŸ¥è­˜ãƒ»å‰ææ¡ä»¶
- ã‚ˆãã‚ã‚‹èª¤è§£ã®è§£æ¶ˆ

### H2: {main_keyword}é¸ã³ã§å¤±æ•—ã—ãŒã¡ãª3ã¤ã®ãƒã‚¤ãƒ³ãƒˆ
1. {reader_analysis["pain_points"][0] if len(reader_analysis["pain_points"]) > 0 else "ã‚ˆãã‚ã‚‹å¤±æ•—"}
2. {reader_analysis["pain_points"][1] if len(reader_analysis["pain_points"]) > 1 else "æ³¨æ„ã™ã¹ããƒã‚¤ãƒ³ãƒˆ"}
3. {reader_analysis["pain_points"][2] if len(reader_analysis["pain_points"]) > 2 else "è¦‹è½ã¨ã—ãŒã¡ãªè¦ç´ "}

### H2: {related_keywords[0] if related_keywords else "é‡è¦é …ç›®"}ã‹ã‚‰è€ƒãˆã‚‹é¸ã³æ–¹
- å…·ä½“çš„ãªé¸æŠåŸºæº–
- åˆ¤æ–­ã®ãƒã‚¤ãƒ³ãƒˆ

### H2: {related_keywords[1] if len(related_keywords) > 1 else "äºˆç®—ãƒ»ä¾¡æ ¼å¸¯"}ã§é¸ã¶{main_keyword}
- ä¾¡æ ¼å¸¯åˆ¥ã®ç‰¹å¾´
- ã‚³ã‚¹ãƒˆãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹é‡è¦–ã®é¸ã³æ–¹

### H2: ãŠã™ã™ã‚{main_keyword} å³é¸5é¸
1. å•†å“å1ï¼šï¼ˆç‰¹å¾´ãƒ»ãŠã™ã™ã‚ãƒã‚¤ãƒ³ãƒˆï¼‰
2. å•†å“å2ï¼šï¼ˆç‰¹å¾´ãƒ»ãŠã™ã™ã‚ãƒã‚¤ãƒ³ãƒˆï¼‰
3. å•†å“å3ï¼šï¼ˆç‰¹å¾´ãƒ»ãŠã™ã™ã‚ãƒã‚¤ãƒ³ãƒˆï¼‰
4. å•†å“å4ï¼šï¼ˆç‰¹å¾´ãƒ»ãŠã™ã™ã‚ãƒã‚¤ãƒ³ãƒˆï¼‰
5. å•†å“å5ï¼šï¼ˆç‰¹å¾´ãƒ»ãŠã™ã™ã‚ãƒã‚¤ãƒ³ãƒˆï¼‰

### H2: {main_keyword}é¸ã³ã§ã‚ˆãã‚ã‚‹è³ªå•Q&A
- Q1: {reader_analysis["primary_concerns"][0] if reader_analysis["primary_concerns"] else "ã‚ˆãã‚ã‚‹è³ªå•"}
- Q2: äºˆç®—ã¯ã©ã®ãã‚‰ã„å¿…è¦ï¼Ÿ
- Q3: åˆå¿ƒè€…ã§ã‚‚å¤§ä¸ˆå¤«ï¼Ÿ

### H2: ã¾ã¨ã‚ï¼šå¤±æ•—ã—ãªã„{main_keyword}ã®é¸ã³æ–¹
- é‡è¦ãƒã‚¤ãƒ³ãƒˆã®å†æ•´ç†
- æœ€çµ‚çš„ãªé¸æŠã®æ±ºã‚æ‰‹
- æ¬¡ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ææ¡ˆ"""
        
        return structure
    
    def save_skeleton_file(self, skeleton_content: str, input_data: Dict) -> str:
        """éª¨æ ¼ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆä¸­ãƒ•ã‚©ãƒ«ãƒ€ã«ä¿å­˜"""
        
        # ãƒ•ã‚¡ã‚¤ãƒ«åç”Ÿæˆ
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        main_keyword_clean = re.sub(r'[^\w\s-]', '', input_data["main_keyword"]).strip()
        main_keyword_clean = re.sub(r'[\s_]+', '_', main_keyword_clean)
        
        filename = f"{timestamp}_{main_keyword_clean}_éª¨æ ¼.md"
        
        # ä¿å­˜
        os.makedirs(self.work_in_progress_path, exist_ok=True)
        file_path = os.path.join(self.work_in_progress_path, filename)
        
        with open(file_path, 'w', encoding='utf-8') as f:
            f.write(skeleton_content)
        
        return file_path

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    system = CLIAutoWritingSystem()
    
    print("ğŸš€ CLIè‡ªå‹•è¨˜äº‹ä½œæˆã‚·ã‚¹ãƒ†ãƒ  - ãƒ©ã‚¤ãƒ†ã‚£ãƒ³ã‚°æ¡ˆä»¶ï¼ˆãƒŸãƒãƒ«ãƒ´ã‚¹ãƒªãƒ¼ãƒ—ï¼‰")
    print("=" * 80)
    
    # FINALç‰ˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆç¢ºèª
    template = system.load_final_template()
    if not template:
        print("âŒ FINALç‰ˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆãŒèª­ã¿è¾¼ã‚ã¾ã›ã‚“")
        return
    
    print("âœ… FINALç‰ˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆèª­ã¿è¾¼ã¿å®Œäº†")
    
    # å‚ç…§è¨˜äº‹èª­ã¿è¾¼ã¿ï¼ˆPhase 0ï¼‰
    references = system.load_reference_articles()
    print(f"âœ… å‚ç…§è¨˜äº‹èª­ã¿è¾¼ã¿å®Œäº†ï¼ˆ{len(references)}ä»¶ï¼‰")
    
    # å…¥åŠ›è³‡æ–™å—ã‘å–ã‚Š
    print("\nğŸ“ ç«¶åˆåˆ†æãƒ»ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰è³‡æ–™ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ï¼š")
    print("ï¼ˆä¾‹ï¼šã€ãƒ¡ã‚¤ãƒ³ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã€‘ï¼šãƒ™ãƒƒãƒ‰ãƒ•ãƒ¬ãƒ¼ãƒ  é¸ã³æ–¹ï¼‰")
    print("å…¥åŠ›å®Œäº†å¾Œã€ç©ºè¡Œã‚’å…¥ã‚Œã¦ENTERã‚’æŠ¼ã—ã¦ãã ã•ã„")
    print("-" * 50)
    
    input_lines = []
    empty_lines = 0
    
    while True:
        try:
            line = input()
            if line.strip() == "":
                empty_lines += 1
                if empty_lines >= 2:  # 2å›é€£ç¶šç©ºè¡Œã§çµ‚äº†
                    break
            else:
                empty_lines = 0
                input_lines.append(line)
        except EOFError:
            break
        except KeyboardInterrupt:
            print("\nâŒ å…¥åŠ›ã‚’ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã—ã¾ã—ãŸ")
            return
    
    input_text = '\n'.join(input_lines)
    
    if not input_text.strip():
        print("âŒ å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ãŒç©ºã§ã™")
        return
    
    # ãƒ‡ãƒ¼ã‚¿è§£æ
    print("\nğŸ” å…¥åŠ›ãƒ‡ãƒ¼ã‚¿è§£æä¸­...")
    input_data = system.parse_input_data(input_text)
    
    if not input_data.get("main_keyword"):
        print("âŒ ãƒ¡ã‚¤ãƒ³ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        return
    
    print(f"âœ… è§£æå®Œäº†:")
    print(f"   ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ: {input_data.get('project', 'ãªã—')}")
    print(f"   ãƒ¡ã‚¤ãƒ³ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {input_data['main_keyword']}")
    print(f"   é–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {', '.join(input_data['related_keywords'])}")
    
    # + Î± ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ã‚·ã‚¹ãƒ†ãƒ çµ±åˆåˆ†æ
    archive_analysis = None
    if system.archive_system:
        try:
            print("\nğŸ” çµ±åˆã‚¢ãƒ¼ã‚«ã‚¤ãƒ–åˆ†æå®Ÿè¡Œä¸­...")
            user_request = f"ãƒ¡ã‚¤ãƒ³ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {input_data['main_keyword']}, é–¢é€£: {', '.join(input_data['related_keywords'])}"
            archive_analysis = system.archive_system.auto_archive_utilization_workflow(user_request)
            if archive_analysis["workflow_status"] == "success":
                print("âœ… ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–åˆ†æå®Œäº†")
                if archive_analysis["duplication_check"]["status"] != "SAFE":
                    print(f"âš ï¸ é‡è¤‡ãƒªã‚¹ã‚¯æ¤œå‡º: {archive_analysis['duplication_check']['status']}")
                    if archive_analysis["duplication_check"]["suggestions"]:
                        print(f"ğŸ’¡ å·®åˆ¥åŒ–ææ¡ˆ: {archive_analysis['duplication_check']['suggestions'][0]}")
                        
                        # é‡è¤‡ãƒªã‚¹ã‚¯ãŒé«˜ã„å ´åˆã¯ç¢ºèªã‚’æ±‚ã‚ã‚‹
                        if archive_analysis["duplication_check"]["status"] == "HIGH_RISK":
                            continue_confirm = input(f"\nâš ï¸ é«˜ã„é‡è¤‡ãƒªã‚¹ã‚¯ãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸã€‚ç¶šè¡Œã—ã¾ã™ã‹ï¼Ÿ (y/n): ").lower().strip()
                            if continue_confirm != 'y':
                                print("âŒ è¨˜äº‹ä½œæˆã‚’ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã—ã¾ã—ãŸ")
                                return
            else:
                print("âš ï¸ ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–åˆ†æã§å•é¡ŒãŒç™ºç”Ÿã—ã¾ã—ãŸ")
        except Exception as e:
            print(f"âš ï¸ ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–åˆ†æã‚¨ãƒ©ãƒ¼: {e}")
    
    # èª­è€…ãƒ‹ãƒ¼ã‚ºåˆ†æ
    print("\nğŸ¯ èª­è€…ãƒ‹ãƒ¼ã‚ºåˆ†æå®Ÿè¡Œä¸­...")
    reader_analysis = system.analyze_reader_needs(
        input_data["main_keyword"], 
        input_data["related_keywords"]
    )
    
    # è¨˜äº‹æ§‹é€ ä½œæˆ
    skeleton = system.create_article_structure(input_data, reader_analysis, references)
    
    # ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼è¡¨ç¤º
    print("\n" + "="*80)
    print("ğŸ“‹ ä½œæˆã•ã‚ŒãŸè¨˜äº‹éª¨æ ¼ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼:")
    print("="*80)
    print(skeleton[:1500] + "..." if len(skeleton) > 1500 else skeleton)
    print("="*80)
    
    # ä¿å­˜ç¢ºèª
    save_confirm = input("\nâœ… ã“ã®éª¨æ ¼ã‚’ã€Œ3_ä½œæˆä¸­ã€ãƒ•ã‚©ãƒ«ãƒ€ã«ä¿å­˜ã—ã¾ã™ã‹ï¼Ÿ (y/n): ").lower().strip()
    
    if save_confirm == 'y':
        try:
            file_path = system.save_skeleton_file(skeleton, input_data)
            print(f"âœ… éª¨æ ¼ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜å®Œäº†:")
            print(f"   ãƒ•ã‚¡ã‚¤ãƒ«: {os.path.basename(file_path)}")
            print(f"   ãƒ‘ã‚¹: {file_path}")
            print("\nğŸ“‹ æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—:")
            print("1. ä¿å­˜ã•ã‚ŒãŸéª¨æ ¼ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ¬ãƒ“ãƒ¥ãƒ¼")
            print("2. ä¿®æ­£ãƒ»æ‰¿èªå¾Œã€æ–‡ç« ä½œæˆã‚’é–‹å§‹")
            print("3. å®Œæˆå¾Œã¯ã€Œ2_å®Œæˆè¨˜äº‹ã€ãƒ•ã‚©ãƒ«ãƒ€ã¸ç§»å‹•")
            
        except Exception as e:
            print(f"âŒ ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
    else:
        print("âŒ ä¿å­˜ã‚’ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã—ã¾ã—ãŸ")

if __name__ == "__main__":
    main()