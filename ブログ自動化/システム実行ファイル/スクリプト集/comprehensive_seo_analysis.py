"""
ãƒ–ãƒ­ã‚°ã®çœŸã®SEOèª²é¡Œã‚’åŒ…æ‹¬çš„ã«åˆ†æ
ãƒ¡ã‚¿ãƒ‡ã‚£ã‚¹ã‚¯ãƒªãƒ—ã‚·ãƒ§ãƒ³ä»¥å¤–ã®æ ¹æœ¬çš„SEOå•é¡Œã‚’ç‰¹å®š
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from core.wordpress_api import WordPressBlogAutomator
import requests
import re

def comprehensive_seo_analysis(post_id):
    """è¨˜äº‹ã®åŒ…æ‹¬çš„SEOåˆ†æ"""
    
    wp = WordPressBlogAutomator()  # ç’°å¢ƒå¤‰æ•°ã‹ã‚‰è‡ªå‹•èª­ã¿è¾¼ã¿
    
    response = requests.get(f'{wp.api_url}/posts/{post_id}', headers=wp.headers)
    if response.status_code != 200:
        return None
        
    post = response.json()
    title = post['title']['rendered']
    content = post['content']['rendered']
    
    print(f'ğŸ” åŒ…æ‹¬çš„SEOåˆ†æ: {title}')
    print('='*80)
    
    # 1. ã‚¿ã‚¤ãƒˆãƒ«ã‚¿ã‚°æˆ¦ç•¥åˆ†æ
    title_length = len(title)
    print(f'ğŸ“ ã‚¿ã‚¤ãƒˆãƒ«æˆ¦ç•¥åˆ†æ:')
    print(f'   æ–‡å­—æ•°: {title_length}æ–‡å­— (æ¨å¥¨: 28-35æ–‡å­—)')
    
    # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰é…ç½®åˆ†æ
    title_words = title.split()
    main_keyword_position = next((i for i, word in enumerate(title_words) if 'Audible' in word), -1)
    print(f'   ãƒ¡ã‚¤ãƒ³ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ä½ç½®: {main_keyword_position + 1}ç•ªç›® (æ¨å¥¨: 1-3ç•ªç›®)')
    
    # æ„Ÿæƒ…è¨´æ±‚ãƒ»ã‚¯ãƒªãƒƒã‚¯èª˜ç™ºè¦ç´ 
    emotional_triggers = ['ä¸–ç•Œä¸€', 'å®Œå…¨', 'å¾¹åº•', 'å®Ÿéš›', 'æœ¬å½“']
    trigger_count = sum(1 for trigger in emotional_triggers if trigger in title)
    print(f'   æ„Ÿæƒ…è¨´æ±‚è¦ç´ : {trigger_count}å€‹å«æœ‰')
    
    # 2. è¦‹å‡ºã—æ§‹é€ ã¨SEOæœ€é©åŒ–åˆ†æ
    h1_tags = re.findall(r'<h1[^>]*>(.*?)</h1>', content, re.DOTALL)
    h2_tags = re.findall(r'<h2[^>]*>(.*?)</h2>', content, re.DOTALL)
    h3_tags = re.findall(r'<h3[^>]*>(.*?)</h3>', content, re.DOTALL)
    
    print(f'\nğŸ—ï¸ è¦‹å‡ºã—æ§‹é€ ã¨ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æœ€é©åŒ–:')
    print(f'   H1: {len(h1_tags)}å€‹ (æ¨å¥¨: 1å€‹)')
    print(f'   H2: {len(h2_tags)}å€‹')
    print(f'   H3: {len(h3_tags)}å€‹')
    
    # H2è¦‹å‡ºã—ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰å«æœ‰ç‡
    if h2_tags:
        keyword_h2_count = 0
        print(f'   H2è¦‹å‡ºã—åˆ†æ:')
        for i, h2 in enumerate(h2_tags[:5]):
            clean_h2 = re.sub(r'<[^>]+>', '', h2).strip()
            has_keyword = any(kw in clean_h2 for kw in ['Audible', 'ãŠé‡‘', 'æŠ•è³‡', 'ç¯€ç´„'])
            if has_keyword:
                keyword_h2_count += 1
            print(f'     H{i+1}: {clean_h2} {"âœ…" if has_keyword else "âŒ"}')
        
        keyword_ratio = keyword_h2_count / len(h2_tags) * 100
        print(f'   H2ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰å«æœ‰ç‡: {keyword_ratio:.1f}% (æ¨å¥¨: 60%ä»¥ä¸Š)')
    
    # 3. ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æˆ¦ç•¥ã¨å¯†åº¦åˆ†æ
    clean_content = re.sub(r'<[^>]+>', '', content)
    word_count = len(clean_content)
    
    print(f'\nğŸ¯ ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æˆ¦ç•¥åˆ†æ:')
    
    # ãƒ¡ã‚¤ãƒ³ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ç¾¤ã®åˆ†æ
    primary_keywords = {
        'Audible': ['Audible', 'ã‚ªãƒ¼ãƒ‡ã‚£ãƒ–ãƒ«'],
        'ãŠé‡‘é–¢é€£': ['ãŠé‡‘', 'æŠ•è³‡', 'ç¯€ç´„', 'è²¯é‡‘', 'è³‡ç”£'],
        'ãŠã™ã™ã‚': ['ãŠã™ã™ã‚', 'å³é¸', 'ãƒ™ã‚¹ãƒˆ'],
        'å­¦ç¿’': ['å‹‰å¼·', 'å­¦ç¿’', 'çŸ¥è­˜', 'ã‚¹ã‚­ãƒ«']
    }
    
    for category, keywords in primary_keywords.items():
        total_count = sum(clean_content.count(kw) for kw in keywords)
        density = (total_count / word_count * 100) if word_count > 0 else 0
        
        # å†’é ­1000æ–‡å­—ã§ã®å‡ºç¾
        early_count = sum(clean_content[:1000].count(kw) for kw in keywords)
        
        print(f'   {category}: {total_count}å› ({density:.2f}%) - å†’é ­{early_count}å›')
        
        # æ¨å¥¨å¯†åº¦ãƒã‚§ãƒƒã‚¯
        if category == 'Audible' and (density < 0.5 or density > 3.0):
            print(f'     âš ï¸ ãƒ¡ã‚¤ãƒ³ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰å¯†åº¦è¦èª¿æ•´ (æ¨å¥¨: 0.5-3.0%)')
    
    # 4. å†…éƒ¨ãƒªãƒ³ã‚¯æˆ¦ç•¥åˆ†æ
    internal_links = re.findall(r'<a[^>]*href=["\']https://muffin-blog\.com[^"\']*["\']*[^>]*>', content)
    external_links = re.findall(r'<a[^>]*href=["\']https?://(?!muffin-blog\.com)[^"\']*["\']*[^>]*>', content)
    
    print(f'\nğŸ”— ãƒªãƒ³ã‚¯æˆ¦ç•¥åˆ†æ:')
    print(f'   å†…éƒ¨ãƒªãƒ³ã‚¯: {len(internal_links)}å€‹')
    print(f'   å¤–éƒ¨ãƒªãƒ³ã‚¯: {len(external_links)}å€‹')
    
    # Amazon ã‚¢ãƒ•ã‚£ãƒªã‚¨ã‚¤ãƒˆãƒªãƒ³ã‚¯åˆ†æ
    amazon_links = len(re.findall(r'amazon\.|amzn\.', content))
    print(f'   Amazonãƒªãƒ³ã‚¯: {amazon_links}å€‹')
    
    # ãƒªãƒ³ã‚¯ãƒãƒ©ãƒ³ã‚¹è©•ä¾¡
    total_links = len(internal_links) + len(external_links)
    if total_links > 0:
        internal_ratio = len(internal_links) / total_links * 100
        print(f'   å†…éƒ¨ãƒªãƒ³ã‚¯æ¯”ç‡: {internal_ratio:.1f}% (æ¨å¥¨: 70-80%)')
    
    # 5. ç”»åƒæœ€é©åŒ–ã¨ãƒ¡ãƒ‡ã‚£ã‚¢æˆ¦ç•¥
    images = re.findall(r'<img[^>]*>', content)
    alt_attributes = re.findall(r'alt=["\']([^"\']*)["\']', content)
    
    print(f'\nğŸ–¼ï¸ ç”»åƒãƒ»ãƒ¡ãƒ‡ã‚£ã‚¢æœ€é©åŒ–:')
    print(f'   ç”»åƒæ•°: {len(images)}å€‹')
    print(f'   altå±æ€§è¨­å®š: {len(alt_attributes)}å€‹')
    
    if images:
        alt_optimization_rate = len(alt_attributes) / len(images) * 100
        print(f'   altæœ€é©åŒ–ç‡: {alt_optimization_rate:.1f}% (æ¨å¥¨: 100%)')
        
        # altå±æ€§ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰å«æœ‰ãƒã‚§ãƒƒã‚¯
        keyword_alts = sum(1 for alt in alt_attributes if any(kw in alt for kw in ['Audible', 'ãŠé‡‘', 'æŠ•è³‡']))
        if alt_attributes:
            keyword_alt_ratio = keyword_alts / len(alt_attributes) * 100
            print(f'   altã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰å«æœ‰ç‡: {keyword_alt_ratio:.1f}%')
    
    # 6. ã‚³ãƒ³ãƒ†ãƒ³ãƒ„å“è³ªã¨ãƒ¦ãƒ¼ã‚¶ãƒ“ãƒªãƒ†ã‚£
    paragraphs = re.findall(r'<p[^>]*>(.*?)</p>', content, re.DOTALL)
    
    print(f'\nğŸ“Š ã‚³ãƒ³ãƒ†ãƒ³ãƒ„å“è³ªåˆ†æ:')
    print(f'   ç·æ–‡å­—æ•°: {word_count:,}æ–‡å­—')
    print(f'   æ®µè½æ•°: {len(paragraphs)}å€‹')
    
    if paragraphs:
        avg_paragraph_length = sum(len(re.sub(r'<[^>]+>', '', p)) for p in paragraphs) / len(paragraphs)
        print(f'   å¹³å‡æ®µè½é•·: {avg_paragraph_length:.0f}æ–‡å­— (æ¨å¥¨: 100-200æ–‡å­—)')
    
    # èª­ã¿ã‚„ã™ã•æŒ‡æ¨™
    sentence_count = clean_content.count('ã€‚') + clean_content.count('ï¼') + clean_content.count('ï¼Ÿ')
    if sentence_count > 0:
        avg_sentence_length = word_count / sentence_count
        print(f'   å¹³å‡æ–‡é•·: {avg_sentence_length:.0f}æ–‡å­— (æ¨å¥¨: 40-60æ–‡å­—)')
    
    # 7. æ¤œç´¢æ„å›³é©åˆåº¦åˆ†æ
    print(f'\nğŸ¯ æ¤œç´¢æ„å›³é©åˆåº¦:')
    
    # æƒ…å ±å‹æ¤œç´¢æ„å›³
    info_keywords = ['æ–¹æ³•', 'ã‚„ã‚Šæ–¹', 'å§‹ã‚æ–¹', 'ã¨ã¯', 'ã«ã¤ã„ã¦', 'è§£èª¬']
    info_score = sum(clean_content.count(kw) for kw in info_keywords)
    
    # å•†ç”¨æ¤œç´¢æ„å›³
    commercial_keywords = ['ãŠã™ã™ã‚', 'æ¯”è¼ƒ', 'ãƒ¬ãƒ“ãƒ¥ãƒ¼', 'å£ã‚³ãƒŸ', 'è©•åˆ¤', 'é¸ã³æ–¹']
    commercial_score = sum(clean_content.count(kw) for kw in commercial_keywords)
    
    # å–å¼•å‹æ¤œç´¢æ„å›³
    transactional_keywords = ['è³¼å…¥', 'ç™»éŒ²', 'ç”³è¾¼', 'ç„¡æ–™', 'ãŠè©¦ã—']
    transactional_score = sum(clean_content.count(kw) for kw in transactional_keywords)
    
    print(f'   æƒ…å ±å‹ã‚¹ã‚³ã‚¢: {info_score}ç‚¹')
    print(f'   å•†ç”¨å‹ã‚¹ã‚³ã‚¢: {commercial_score}ç‚¹')
    print(f'   å–å¼•å‹ã‚¹ã‚³ã‚¢: {transactional_score}ç‚¹')
    
    # ä¸»è¦æ¤œç´¢æ„å›³ã®åˆ¤å®š
    max_score = max(info_score, commercial_score, transactional_score)
    if max_score == commercial_score:
        print(f'   â†’ å•†ç”¨æ¤œç´¢æ„å›³ã«æœ€é©åŒ–æ¸ˆã¿')
    elif max_score == info_score:
        print(f'   â†’ æƒ…å ±æ¤œç´¢æ„å›³ã«æœ€é©åŒ–æ¸ˆã¿')
    else:
        print(f'   â†’ å–å¼•æ¤œç´¢æ„å›³ã«æœ€é©åŒ–æ¸ˆã¿')
    
    # 8. ç«¶åˆå„ªä½æ€§ã¨ç‹¬è‡ªä¾¡å€¤
    print(f'\nğŸ† ç«¶åˆå„ªä½æ€§åˆ†æ:')
    
    unique_value_indicators = {
        'å®Ÿä½“é¨“': ['å®Ÿéš›ã«', 'ä½“é¨“', 'ä½¿ã£ã¦ã¿ãŸ', 'è©¦ã—ãŸ'],
        'å°‚é–€æ€§': ['å°‚é–€', 'ãƒ—ãƒ­', 'è©³ã—ã', 'å¾¹åº•'],
        'ç¶²ç¾…æ€§': ['å®Œå…¨', 'å…¨ã¦', 'ç·ã¾ã¨ã‚', 'ä¸€è¦§'],
        'æœ€æ–°æ€§': ['2024', 'æœ€æ–°', 'ã‚¢ãƒƒãƒ—ãƒ‡ãƒ¼ãƒˆ', 'ç¾åœ¨'],
        'åˆå¿ƒè€…é…æ…®': ['åˆå¿ƒè€…', 'ã¯ã˜ã‚ã¦', 'åˆ†ã‹ã‚Šã‚„ã™ã', 'ç°¡å˜']
    }
    
    for indicator, keywords in unique_value_indicators.items():
        count = sum(clean_content.count(kw) for kw in keywords)
        if count > 0:
            print(f'   âœ… {indicator}: {count}ç®‡æ‰€ã§è¨€åŠ')
    
    # 9. ãƒ†ã‚¯ãƒ‹ã‚«ãƒ«SEOèª²é¡Œ
    print(f'\nâš™ï¸ ãƒ†ã‚¯ãƒ‹ã‚«ãƒ«SEOèª²é¡Œ:')
    
    # æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ã®ç¢ºèª
    json_ld = re.findall(r'<script[^>]*type=["\']application/ld\+json["\'][^>]*>', content)
    print(f'   æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿: {len(json_ld)}å€‹')
    
    # ç›®æ¬¡ã®æœ‰ç„¡
    toc_indicators = ['ç›®æ¬¡', 'ã“ã®è¨˜äº‹ã§åˆ†ã‹ã‚‹ã“ã¨', 'ã‚‚ãã˜']
    has_toc = any(indicator in clean_content for indicator in toc_indicators)
    print(f'   ç›®æ¬¡è¨­ç½®: {"âœ…" if has_toc else "âŒ"}')
    
    return True

if __name__ == "__main__":
    print('ğŸš€ ãƒ–ãƒ­ã‚°ã®çœŸã®SEOèª²é¡Œ åŒ…æ‹¬åˆ†æ')
    print('ãƒ¡ã‚¿ãƒ‡ã‚£ã‚¹ã‚¯ãƒªãƒ—ã‚·ãƒ§ãƒ³ä»¥å¤–ã®æ ¹æœ¬çš„å•é¡Œã‚’ç‰¹å®š')
    print('='*80)
    
    # ä¸»è¦è¨˜äº‹ã‚’å¾¹åº•åˆ†æ
    target_posts = [2732, 2677, 2535]  # ãŠé‡‘ã®å‹‰å¼·ã€ä¼‘ä¼šåˆ¶åº¦ã€å§‹ã‚æ–¹
    
    for post_id in target_posts:
        comprehensive_seo_analysis(post_id)
        print('\n' + '='*80 + '\n')
    
    print('ğŸ“‹ åˆ†æå®Œäº†: çœŸã®SEOæ”¹å–„ãƒã‚¤ãƒ³ãƒˆã‚’ç‰¹å®šã—ã¾ã—ãŸ')